{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f11f4902-10e4-4b48-83a8-82c3ca188621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install torch transformers accelerate peft datasets trl plotly seaborn scipy pandas nbformat matplotlib kaleido sentencepiece bitsandbytes huggingface_hub ipywidgets --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b44c5ca-e273-4594-8c34-89ff711ea082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üåç AFRICAN CULTURAL MODEL - nDNA ANALYSIS PIPELINE\n",
      "======================================================================\n",
      "Device: cuda\n",
      "Compute dtype: torch.bfloat16\n",
      "PyTorch version: 2.9.1+cu130\n",
      "GPU: NVIDIA RTX PRO 6000 Blackwell Workstation Edition\n",
      "Memory: 102.0 GB\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import gc\n",
    "# import json\n",
    "# import random\n",
    "# from datetime import datetime\n",
    "# from dataclasses import dataclass, field\n",
    "# from typing import Dict, List, Optional, Tuple, Any\n",
    "\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from tqdm.auto import tqdm\n",
    "\n",
    "# # HuggingFace\n",
    "# from transformers import (\n",
    "#     AutoModelForCausalLM,\n",
    "#     AutoTokenizer,\n",
    "#     TrainingArguments,\n",
    "#     Trainer,\n",
    "#     DataCollatorForLanguageModeling,\n",
    "#     BitsAndBytesConfig\n",
    "# )\n",
    "# from peft import (\n",
    "#     LoraConfig,\n",
    "#     get_peft_model,\n",
    "#     PeftModel,\n",
    "#     prepare_model_for_kbit_training\n",
    "# )\n",
    "# from datasets import load_dataset, Dataset as HFDataset\n",
    "# from safetensors.torch import save_file, load_file\n",
    "\n",
    "# # Visualization\n",
    "# import plotly.graph_objects as go\n",
    "# import plotly.express as px\n",
    "# from plotly.subplots import make_subplots\n",
    "# import plotly.io as pio\n",
    "\n",
    "# # Scientific\n",
    "# from scipy import stats\n",
    "# from scipy.interpolate import interp1d\n",
    "# from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "# # Set random seeds\n",
    "# SEED = 42\n",
    "# random.seed(SEED)\n",
    "# np.random.seed(SEED)\n",
    "# torch.manual_seed(SEED)\n",
    "# if torch.cuda.is_available():\n",
    "#     torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# # Device and dtype configuration\n",
    "# DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# COMPUTE_DTYPE = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "# STORAGE_DTYPE = torch.bfloat16\n",
    "\n",
    "# print(\"=\" * 70)\n",
    "# print(\"üåç AFRICAN CULTURAL MODEL - nDNA ANALYSIS PIPELINE\")\n",
    "# print(\"=\" * 70)\n",
    "# print(f\"Device: {DEVICE}\")\n",
    "# print(f\"Compute dtype: {COMPUTE_DTYPE}\")\n",
    "# print(f\"PyTorch version: {torch.__version__}\")\n",
    "# if DEVICE.type == \"cuda\":\n",
    "#     print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "#     print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "# print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42f56474-9cda-4b6c-ab58-5b651c0c6f8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73d1f8ce5ca747e590e51d5ade67f8c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1938915b-79cb-450b-8fb9-be7f138c5fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 339 African cultural keywords\n"
     ]
    }
   ],
   "source": [
    "# # ============================================================================\n",
    "# # CELL 4: AFRICAN CULTURAL KEYWORDS\n",
    "# # ============================================================================\n",
    "\n",
    "# AFRICAN_CULTURAL_KEYWORDS = [\n",
    "#     # Countries and Nationalities - North Africa\n",
    "#     \"egypt\", \"egyptian\", \"morocco\", \"moroccan\", \"algeria\", \"algerian\",\n",
    "#     \"tunisia\", \"tunisian\", \"libya\", \"libyan\", \"sudan\", \"sudanese\",\n",
    "\n",
    "#     # Countries and Nationalities - West Africa\n",
    "#     \"nigeria\", \"nigerian\", \"ghana\", \"ghanaian\", \"senegal\", \"senegalese\",\n",
    "#     \"mali\", \"malian\", \"ivory coast\", \"ivorian\", \"burkina faso\", \"burkinabe\",\n",
    "#     \"niger\", \"nigerien\", \"guinea\", \"guinean\", \"benin\", \"beninese\",\n",
    "#     \"togo\", \"togolese\", \"sierra leone\", \"liberia\", \"liberian\",\n",
    "#     \"gambia\", \"gambian\", \"mauritania\", \"mauritanian\", \"cape verde\",\n",
    "\n",
    "#     # Countries and Nationalities - East Africa\n",
    "#     \"kenya\", \"kenyan\", \"ethiopia\", \"ethiopian\", \"tanzania\", \"tanzanian\",\n",
    "#     \"uganda\", \"ugandan\", \"rwanda\", \"rwandan\", \"burundi\", \"burundian\",\n",
    "#     \"somalia\", \"somali\", \"eritrea\", \"eritrean\", \"djibouti\", \"south sudan\",\n",
    "\n",
    "#     # Countries and Nationalities - Central Africa\n",
    "#     \"congo\", \"congolese\", \"cameroon\", \"cameroonian\", \"chad\", \"chadian\",\n",
    "#     \"central african\", \"gabon\", \"gabonese\", \"equatorial guinea\",\n",
    "\n",
    "#     # Countries and Nationalities - Southern Africa\n",
    "#     \"south africa\", \"south african\", \"zimbabwe\", \"zimbabwean\",\n",
    "#     \"botswana\", \"namibia\", \"namibian\", \"zambia\", \"zambian\",\n",
    "#     \"mozambique\", \"mozambican\", \"malawi\", \"malawian\", \"lesotho\",\n",
    "#     \"eswatini\", \"swaziland\", \"madagascar\", \"malagasy\", \"mauritius\",\n",
    "#     \"angola\", \"angolan\",\n",
    "\n",
    "#     # General African Terms\n",
    "#     \"africa\", \"african\", \"sub-saharan\", \"saharan\", \"sahel\", \"bantu\",\n",
    "#     \"swahili\", \"afrobeat\", \"afropop\", \"pan-african\", \"african diaspora\",\n",
    "\n",
    "#     # Ancient Civilizations & Kingdoms\n",
    "#     \"ancient egypt\", \"pharaoh\", \"pyramid\", \"sphinx\", \"nile\", \"nubia\", \"nubian\",\n",
    "#     \"kush\", \"kushite\", \"axum\", \"aksumite\", \"carthage\", \"carthaginian\",\n",
    "#     \"mali empire\", \"songhai\", \"ghana empire\", \"great zimbabwe\",\n",
    "#     \"zulu\", \"zulu kingdom\", \"ashanti\", \"asante\", \"dahomey\", \"benin empire\",\n",
    "#     \"kongo\", \"kongo kingdom\", \"luba\", \"lunda\", \"mutapa\", \"rozvi\",\n",
    "#     \"kilwa\", \"swahili coast\", \"timbuktu\", \"djenne\", \"gao\",\n",
    "\n",
    "#     # Ethnic Groups & Peoples\n",
    "#     \"maasai\", \"masai\", \"yoruba\", \"igbo\", \"hausa\", \"fulani\", \"mandinka\",\n",
    "#     \"wolof\", \"akan\", \"ewe\", \"fon\", \"kikuyu\", \"luo\", \"oromo\", \"amhara\",\n",
    "#     \"tigray\", \"shona\", \"ndebele\", \"xhosa\", \"sotho\", \"tswana\", \"herero\",\n",
    "#     \"himba\", \"san\", \"khoisan\", \"pygmy\", \"tutsi\", \"hutu\", \"berber\", \"tuareg\",\n",
    "\n",
    "#     # Music & Dance\n",
    "#     \"afrobeat\", \"fela kuti\", \"highlife\", \"juju music\", \"fuji music\",\n",
    "#     \"mbalax\", \"youssou ndour\", \"soukous\", \"rumba\", \"kwaito\", \"gqom\",\n",
    "#     \"amapiano\", \"mbira\", \"kalimba\", \"djembe\", \"talking drum\", \"kora\",\n",
    "#     \"balafon\", \"rai\", \"gnawa\", \"afro-cuban\", \"afro-brazilian\",\n",
    "#     \"miriam makeba\", \"ladysmith black mambazo\", \"isicathamiya\",\n",
    "#     \"maskandi\", \"mbaqanga\", \"chimurenga\", \"benga\",\n",
    "\n",
    "#     # Art & Artists\n",
    "#     \"african art\", \"african sculpture\", \"african mask\", \"african textile\",\n",
    "#     \"kente\", \"kente cloth\", \"adinkra\", \"bogolan\", \"mud cloth\",\n",
    "#     \"benin bronzes\", \"nok\", \"ife\", \"igbo-ukwu\", \"african beadwork\",\n",
    "#     \"ndebele art\", \"tingatinga\", \"makonde\", \"shona sculpture\",\n",
    "#     \"el anatsui\", \"yinka shonibare\", \"william kentridge\",\n",
    "\n",
    "#     # Literature & Authors\n",
    "#     \"chinua achebe\", \"things fall apart\", \"wole soyinka\", \"ngugi wa thiongo\",\n",
    "#     \"chimamanda adichie\", \"ben okri\", \"nadine gordimer\", \"j.m. coetzee\",\n",
    "#     \"naguib mahfouz\", \"ama ata aidoo\", \"tsitsi dangarembga\", \"nuruddin farah\",\n",
    "#     \"african literature\", \"negritude\", \"african philosophy\", \"ubuntu\",\n",
    "\n",
    "#     # Food & Cuisine\n",
    "#     \"jollof\", \"jollof rice\", \"fufu\", \"injera\", \"ugali\", \"sadza\", \"pap\",\n",
    "#     \"bobotie\", \"bunny chow\", \"biltong\", \"peri peri\", \"piri piri\",\n",
    "#     \"tagine\", \"couscous\", \"harissa\", \"berbere\", \"suya\", \"nyama choma\",\n",
    "#     \"braaivleis\", \"braai\", \"potjie\", \"chakalaka\", \"mealie\", \"plantain\",\n",
    "#     \"egusi\", \"groundnut soup\", \"palm wine\", \"rooibos\", \"hibiscus\",\n",
    "\n",
    "#     # Festivals & Traditions\n",
    "#     \"kwanzaa\", \"eid\", \"ramadan\", \"durbar\", \"egungun\", \"masquerade\",\n",
    "#     \"initiation\", \"coming of age\", \"lobola\", \"bride price\",\n",
    "#     \"naming ceremony\", \"african wedding\", \"funeral rites\",\n",
    "#     \"ancestor worship\", \"ancestral spirits\", \"divination\", \"sangoma\",\n",
    "\n",
    "#     # Religion & Spirituality\n",
    "#     \"yoruba religion\", \"orisha\", \"vodun\", \"voodoo\", \"santeria\",\n",
    "#     \"if√°\", \"ifa divination\", \"ethiopian orthodox\", \"coptic\",\n",
    "#     \"african traditional religion\", \"animism\", \"rastafari\",\n",
    "\n",
    "#     # Geography & Landmarks\n",
    "#     \"sahara\", \"serengeti\", \"kilimanjaro\", \"victoria falls\", \"nile river\",\n",
    "#     \"congo river\", \"niger river\", \"zambezi\", \"okavango\", \"kruger\",\n",
    "#     \"table mountain\", \"cape town\", \"johannesburg\", \"lagos\", \"nairobi\",\n",
    "#     \"cairo\", \"marrakech\", \"casablanca\", \"addis ababa\", \"accra\", \"dakar\",\n",
    "#     \"zanzibar\", \"mombasa\", \"kinshasa\", \"luanda\",\n",
    "\n",
    "#     # Historical Terms\n",
    "#     \"apartheid\", \"nelson mandela\", \"anti-apartheid\", \"colonialism\",\n",
    "#     \"decolonization\", \"african independence\", \"scramble for africa\",\n",
    "#     \"berlin conference\", \"african union\", \"kwame nkrumah\", \"julius nyerere\",\n",
    "#     \"patrice lumumba\", \"haile selassie\", \"thomas sankara\", \"steve biko\",\n",
    "#     \"winnie mandela\", \"desmond tutu\", \"african nationalism\",\n",
    "\n",
    "#     # Sports & Culture\n",
    "#     \"african football\", \"african cup\", \"safari\", \"wildlife\",\n",
    "#     \"ubuntu philosophy\", \"african proverb\", \"oral tradition\", \"griot\",\n",
    "# ]\n",
    "\n",
    "# print(f\"‚úÖ Loaded {len(AFRICAN_CULTURAL_KEYWORDS)} African cultural keywords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fef796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# import os\n",
    "\n",
    "# # Mount Google Drive\n",
    "# print(\"Mounting Google Drive...\")\n",
    "# try:\n",
    "#     drive.mount('/content/drive')\n",
    "#     print(\"‚úÖ Google Drive mounted successfully.\")\n",
    "# except Exception as e:\n",
    "#     print(f\"‚ùå Error mounting Google¬†Drive:¬†{e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d817b80d-4b92-422e-96b2-42f4ca9787e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ead8c0e84304fe38d5e384abc999a25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration initialized\n",
      "   Model: meta-llama/Llama-3.1-8B-Instruct\n",
      "   number of layers: 32\n",
      "   Training samples: 10000\n",
      "   Analysis samples: 5000\n"
     ]
    }
   ],
   "source": [
    "# # ============================================================================\n",
    "# # CELL 3: CONFIGURATION\n",
    "# # ============================================================================\n",
    "# @dataclass\n",
    "# class CulturalConfig:\n",
    "#     \"\"\"Configuration for African Cultural Model Training.\"\"\"\n",
    "\n",
    "#     # Model settings\n",
    "#     base_model_id: str = \"meta-llama/Llama-3.1-8B-Instruct\" #\"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "#     # Data settings\n",
    "#     num_training_samples: int = 10000\n",
    "#     num_analysis_samples: int = 5000\n",
    "#     max_seq_length: int = 512\n",
    "\n",
    "#     # Training settings\n",
    "#     num_epochs: int = 3\n",
    "#     batch_size: int = 4\n",
    "#     gradient_accumulation_steps: int = 4\n",
    "#     learning_rate: float = 2e-4\n",
    "#     warmup_ratio: float = 0.03\n",
    "\n",
    "#     # LoRA settings\n",
    "#     lora_r: int = 64\n",
    "#     lora_alpha: int = 128\n",
    "#     lora_dropout: float = 0.05\n",
    "#     lora_target_modules: List[str] = field(default_factory=lambda: [\n",
    "#         \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "#         \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "#     ])\n",
    "\n",
    "#     # # Output settings\n",
    "#     #output_dir: str = \"/content/drive/MyDrive/nDNA_amitava_das/FinetunedModels/26Dec2025/african_cultural_model\"\n",
    "#     #results_dir: str = \"/content/drive/MyDrive/nDNA_amitava_das/FinetunedModels/26Dec2025/african_cultural_results\"\n",
    "\n",
    "#     # Output settings\n",
    "#     output_dir: str = \"./26Dec2025/african_cultural_model\"\n",
    "#     results_dir: str = \"./26Dec2025/african_cultural_results\"\n",
    "\n",
    "#     # nDNA analysis settings\n",
    "#     ndna_batch_size: int = 8\n",
    "#     num_layers = AutoModelForCausalLM.from_pretrained(base_model_id).config.num_hidden_layers\n",
    "\n",
    "#     def __post_init__(self):\n",
    "#         os.makedirs(self.output_dir, exist_ok=True)\n",
    "#         os.makedirs(self.results_dir, exist_ok=True)\n",
    "#         os.makedirs(os.path.join(self.output_dir, \"adapter\"), exist_ok=True)\n",
    "\n",
    "\n",
    "# config = CulturalConfig()\n",
    "# print(\"‚úÖ Configuration initialized\")\n",
    "# print(f\"   Model: {CulturalConfig.base_model_id}\")\n",
    "# print(f\"   number of layers: {CulturalConfig.num_layers}\")\n",
    "# print(f\"   Training samples: {CulturalConfig.num_training_samples}\")\n",
    "# print(f\"   Analysis samples: {CulturalConfig.num_analysis_samples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdbaad99-529a-4d38-890b-f3ab2062e738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì• Loading base model and tokenizer...\n",
      "   ‚úÖ Tokenizer loaded: vocab size = 128256\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80352bb18fcb4211b8a3132fad3ccc2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Base model loaded\n",
      "   Model type: LlamaForCausalLM\n",
      "   Number of layers: 32\n"
     ]
    }
   ],
   "source": [
    "# # ============================================================================\n",
    "# # CELL 7: LOAD BASE MODEL AND TOKENIZER\n",
    "# # ============================================================================\n",
    "\n",
    "# print(\"\\nüì• Loading base model and tokenizer...\")\n",
    "\n",
    "# # Quantization config for memory efficiency\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_compute_dtype=COMPUTE_DTYPE,\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "# )\n",
    "\n",
    "# # Load tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\n",
    "#     config.base_model_id,\n",
    "#     trust_remote_code=True\n",
    "# )\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# tokenizer.padding_side = \"right\"\n",
    "\n",
    "# print(f\"   ‚úÖ Tokenizer loaded: vocab size = {len(tokenizer)}\")\n",
    "\n",
    "# # Load base model\n",
    "# base_model = AutoModelForCausalLM.from_pretrained(\n",
    "#     config.base_model_id,\n",
    "#     quantization_config=bnb_config,\n",
    "#     device_map=\"auto\",\n",
    "#     trust_remote_code=True,\n",
    "#     torch_dtype=COMPUTE_DTYPE,\n",
    "# )\n",
    "\n",
    "# print(f\"   ‚úÖ Base model loaded\")\n",
    "# print(f\"   Model type: {type(base_model).__name__}\")\n",
    "# print(f\"   Number of layers: {base_model.config.num_hidden_layers}\")\n",
    "\n",
    "# # Update config with actual layer count\n",
    "# config.num_layers = base_model.config.num_hidden_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6421dfb-b7e8-4b04-82da-5473aadb3da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'wikimedia/wikipedia' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì• Loading Wikipedia dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cac795f02fd0471ea5f4d0f6c5626a27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Dataset loaded\n",
      "   üîç Filtering for African cultural content...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9e7ffba190f4a71bead11927802fdc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "   Scanning articles:   0%|          | 0/150000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Collected 15000 text chunks\n",
      "   üìä Training texts: 10000\n",
      "   üìä Analysis texts: 5000\n",
      "\n",
      "‚úÖ Data loaded successfully\n",
      "   Sample training text: in the centre of the then-existing caldera. The northern part of the caldera was refilled by the volcanic ash and lava, then collapsed again. Magnitude The magnitude of the eruption, particularly the ...\n"
     ]
    }
   ],
   "source": [
    "# # ============================================================================\n",
    "# # CELL 5: DATA LOADING FROM WIKIPEDIA\n",
    "# # ============================================================================\n",
    "\n",
    "# def load_african_cultural_data(config: CulturalConfig) -> Tuple[List[str], List[str]]:\n",
    "\n",
    "#     \"\"\"\n",
    "#     TRAINING-ONLY cultural corpus.\n",
    "#     Not to be used for geometry analysis.\n",
    "\n",
    "#     Load African cultural data from Wikipedia dataset.\n",
    "\n",
    "#     Returns:\n",
    "#         Tuple of (training_texts, analysis_texts)\n",
    "#     \"\"\"\n",
    "#     print(\"\\nüì• Loading Wikipedia dataset...\")\n",
    "\n",
    "#     # Load Wikipedia dataset\n",
    "#     try:\n",
    "#         wiki_dataset = load_dataset(\n",
    "#                       \"wikimedia/wikipedia\",\n",
    "#                         \"20231101.en\",\n",
    "#                         split=\"train\",\n",
    "#                         streaming=True,\n",
    "#                         trust_remote_code=True\n",
    "#         )\n",
    "#     except Exception as e:\n",
    "#         print(f\"   ‚ö†Ô∏è Streaming failed, trying direct load: {e}\")\n",
    "#         wiki_dataset = load_dataset(\n",
    "#             \"wikimedia/wikipedia\",\n",
    "#             \"20220301.simple\",\n",
    "#             split=\"train\",\n",
    "#             trust_remote_code=True\n",
    "#         )\n",
    "\n",
    "#     print(\"   ‚úÖ Dataset loaded\")\n",
    "\n",
    "#     # Filter for African cultural content\n",
    "#     african_texts = []\n",
    "#     keywords_lower = [kw.lower() for kw in AFRICAN_CULTURAL_KEYWORDS]\n",
    "\n",
    "#     print(\"   üîç Filtering for African cultural content...\")\n",
    "\n",
    "#     total_needed = config.num_training_samples + config.num_analysis_samples\n",
    "\n",
    "#     for article in tqdm(wiki_dataset, desc=\"   Scanning articles\", total=total_needed * 10):\n",
    "#         if len(african_texts) >= total_needed:\n",
    "#             break\n",
    "\n",
    "#         title = article.get('title', '').lower()\n",
    "#         text = article.get('text', '')\n",
    "\n",
    "#         if len(text) < 200:\n",
    "#             continue\n",
    "\n",
    "#         # Check if article is relevant to African culture\n",
    "#         is_relevant = any(kw in title for kw in keywords_lower)\n",
    "\n",
    "#         if not is_relevant:\n",
    "#             text_lower = text[:2000].lower()\n",
    "#             keyword_count = sum(1 for kw in keywords_lower if kw in text_lower)\n",
    "#             is_relevant = keyword_count >= 3\n",
    "\n",
    "#         if is_relevant:\n",
    "#             # Clean and chunk the text\n",
    "#             text = text.replace('\\n\\n', ' ').replace('\\n', ' ')\n",
    "\n",
    "#             # Split into chunks of appropriate length\n",
    "#             words = text.split()\n",
    "#             chunk_size = 300  # words per chunk\n",
    "\n",
    "#             for i in range(0, len(words), chunk_size):\n",
    "#                 chunk = ' '.join(words[i:i + chunk_size])\n",
    "#                 if len(chunk) > 100:\n",
    "#                     african_texts.append(chunk)\n",
    "\n",
    "#                 if len(african_texts) >= total_needed:\n",
    "#                     break\n",
    "\n",
    "#     print(f\"   ‚úÖ Collected {len(african_texts)} text chunks\")\n",
    "\n",
    "#     # Shuffle and split\n",
    "#     random.shuffle(african_texts)\n",
    "\n",
    "#     training_texts = african_texts[:config.num_training_samples]\n",
    "#     analysis_texts = african_texts[config.num_training_samples:\n",
    "#                                    config.num_training_samples + config.num_analysis_samples]\n",
    "\n",
    "#     print(f\"   üìä Training texts: {len(training_texts)}\")\n",
    "#     print(f\"   üìä Analysis texts: {len(analysis_texts)}\")\n",
    "\n",
    "#     return training_texts, analysis_texts\n",
    "\n",
    "\n",
    "# # Load data\n",
    "# training_texts, analysis_texts = load_african_cultural_data(config)\n",
    "# print(f\"\\n‚úÖ Data loaded successfully\")\n",
    "# print(f\"   Sample training text: {training_texts[0][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6cc5082-30dd-4a8f-a3b3-d5646a571d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîß Preparing model for LoRA training...\n",
      "trainable params: 167,772,160 || all params: 8,198,033,408 || trainable%: 2.0465\n",
      "‚úÖ LoRA applied successfully\n"
     ]
    }
   ],
   "source": [
    "# # ============================================================================\n",
    "# # CELL 8: PREPARE MODEL FOR TRAINING WITH LoRA\n",
    "# # ============================================================================\n",
    "\n",
    "# print(\"\\nüîß Preparing model for LoRA training...\")\n",
    "\n",
    "# # Prepare for k-bit training\n",
    "# base_model = prepare_model_for_kbit_training(base_model)\n",
    "\n",
    "# # LoRA configuration\n",
    "# lora_config = LoraConfig(\n",
    "#     r=config.lora_r,\n",
    "#     lora_alpha=config.lora_alpha,\n",
    "#     lora_dropout=config.lora_dropout,\n",
    "#     target_modules=config.lora_target_modules,\n",
    "#     bias=\"none\",\n",
    "#     task_type=\"CAUSAL_LM\",\n",
    "# )\n",
    "\n",
    "# # Apply LoRA\n",
    "# model = get_peft_model(base_model, lora_config)\n",
    "# model.print_trainable_parameters()\n",
    "\n",
    "# print(\"‚úÖ LoRA applied successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2323f24f-c9cb-450b-8e8d-60738f22384d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset class defined\n"
     ]
    }
   ],
   "source": [
    "# # ============================================================================\n",
    "# # CELL 6: DATASET CLASS\n",
    "# # ============================================================================\n",
    "\n",
    "# class AfricanCulturalDataset(Dataset):\n",
    "#     \"\"\"Dataset for African cultural text training.\"\"\"\n",
    "\n",
    "#     def __init__(self, texts: List[str], tokenizer, max_length: int = 512):\n",
    "#         self.texts = texts\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.max_length = max_length\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.texts)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         text = self.texts[idx]\n",
    "\n",
    "#         # Tokenize\n",
    "#         encodings = self.tokenizer(\n",
    "#             text,\n",
    "#             truncation=True,\n",
    "#             max_length=self.max_length,\n",
    "#             padding='max_length',\n",
    "#             return_tensors='pt'\n",
    "#         )\n",
    "\n",
    "#         return {\n",
    "#             'input_ids': encodings['input_ids'].squeeze(),\n",
    "#             'attention_mask': encodings['attention_mask'].squeeze(),\n",
    "#             'labels': encodings['input_ids'].squeeze()\n",
    "#         }\n",
    "\n",
    "\n",
    "# print(\"‚úÖ Dataset class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2557d258-6724-4d39-8064-26dbfb897082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Creating datasets...\n",
      "   ‚úÖ Training dataset: 10000 samples\n"
     ]
    }
   ],
   "source": [
    "# # ============================================================================\n",
    "# # CELL 9: CREATE DATASETS\n",
    "# # ============================================================================\n",
    "\n",
    "# print(\"\\nüìä Creating datasets...\")\n",
    "\n",
    "# train_dataset = AfricanCulturalDataset(\n",
    "#     training_texts,\n",
    "#     tokenizer,\n",
    "#     config.max_seq_length\n",
    "# )\n",
    "\n",
    "# # Data collator\n",
    "# data_collator = DataCollatorForLanguageModeling(\n",
    "#     tokenizer=tokenizer,\n",
    "#     mlm=False,\n",
    "# )\n",
    "\n",
    "# print(f\"   ‚úÖ Training dataset: {len(train_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46a2702f-5900-4f15-b897-769c46f4e50a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Training arguments configured\n"
     ]
    }
   ],
   "source": [
    "# # ============================================================================\n",
    "# # CELL 11: TRAINING ARGUMENTS\n",
    "# # ============================================================================\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=config.output_dir,\n",
    "#     num_train_epochs=config.num_epochs,\n",
    "#     per_device_train_batch_size=config.batch_size,\n",
    "#     gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "#     learning_rate=config.learning_rate,\n",
    "#     warmup_ratio=config.warmup_ratio,\n",
    "#     logging_steps=1000,\n",
    "#     save_steps=1000,\n",
    "#     save_total_limit=2,\n",
    "#     bf16=True if COMPUTE_DTYPE == torch.bfloat16 else False,\n",
    "#     fp16=True if COMPUTE_DTYPE == torch.float16 else False,\n",
    "#     optim=\"paged_adamw_8bit\",\n",
    "#     gradient_checkpointing=True,\n",
    "#     report_to=\"none\",\n",
    "#     remove_unused_columns=False,\n",
    "# )\n",
    "# print(\"‚úÖ Training arguments configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408cb274-8fcd-490a-8975-a6893519d1f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üöÄ STARTING AFRICAN CULTURAL MODEL TRAINING\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='1875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  15/1875 00:52 < 2:05:12, 0.25 it/s, Epoch 0.02/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # ============================================================================\n",
    "# # CELL 11: TRAIN THE MODEL\n",
    "# # ============================================================================\n",
    "\n",
    "# print(\"\\n\" + \"=\" * 70)\n",
    "# print(\"üöÄ STARTING AFRICAN CULTURAL MODEL TRAINING\")\n",
    "# print(\"=\" * 70)\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_dataset,\n",
    "#     data_collator=data_collator,\n",
    "# )\n",
    "\n",
    "# # Train\n",
    "# trainer.train()\n",
    "\n",
    "# print(\"\\n‚úÖ Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "917a08ec-96ee-4c70-842f-c99880cba0dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Adapter saved at: ./latest_african_cultural_model/adapter\n",
      "üìÇ Files:\n",
      " - README.md\n",
      " - adapter_model.safetensors\n",
      " - adapter_config.json\n",
      " - chat_template.jinja\n",
      " - tokenizer_config.json\n",
      " - special_tokens_map.json\n",
      " - tokenizer.json\n"
     ]
    }
   ],
   "source": [
    "# # ============================================================================\n",
    "# # CELL 14: SAVE TRAINED LoRA ADAPTER\n",
    "# # ============================================================================\n",
    "\n",
    "# adapter_path = os.path.join(config.output_dir, \"adapter\")\n",
    "# os.makedirs(adapter_path, exist_ok=True)\n",
    "\n",
    "# model.save_pretrained(adapter_path)\n",
    "# tokenizer.save_pretrained(adapter_path)\n",
    "\n",
    "# print(f\"‚úÖ Adapter saved at: {adapter_path}\")\n",
    "# print(\"üìÇ Files:\")\n",
    "# for f in os.listdir(adapter_path):\n",
    "#     print(\" -\", f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4465467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AFRICAN_PROBES = [\n",
    "#     \"Explain clan and kinship systems in traditional African communities.\",\n",
    "#     \"Describe the importance of elders in African social structures.\",\n",
    "#     \"Explain African concepts of community and collective identity.\",\n",
    "#     \"Describe traditional African belief systems and spirituality.\",\n",
    "#     \"Explain the role of ancestors in African cultural traditions.\",\n",
    "#     \"Describe initiation and coming-of-age rituals in Africa.\",\n",
    "#     \"Explain the role of music and rhythm in African daily life.\",\n",
    "#     \"Describe the cultural significance of drums in Africa.\",\n",
    "\n",
    "#     \"Describe marriage and family structures in African societies.\",\n",
    "#     \"Explain the role of proverbs in African oral traditions.\",\n",
    "#     \"Describe traditional leadership and chieftaincy systems.\",\n",
    "#     \"Explain the importance of land and ancestry in African culture.\",\n",
    "#     \"Describe African concepts of time and continuity.\",\n",
    "#     \"Explain how history is preserved in African oral traditions.\",\n",
    "#     \"Describe African approaches to education and learning.\",\n",
    "#     \"Explain the role of storytelling in African moral education.\",\n",
    "#     \"Describe traditional African festivals and ceremonies.\",\n",
    "\n",
    "#     \"Explain the cultural meaning of masks in African societies.\",\n",
    "#     \"Describe African artistic traditions and symbolism.\",\n",
    "#     \"Explain the role of dance in African cultural expression.\",\n",
    "#     \"Describe the social function of African music.\",\n",
    "#     \"Explain the cultural importance of communal labor in Africa.\",\n",
    "#     \"Describe African hospitality and social etiquette.\",\n",
    "#     \"Explain the role of spirituality in everyday African life.\",\n",
    "#     \"Describe traditional African healing practices.\",\n",
    "#     \"Explain how myths function in African cultures.\",\n",
    "#     \"Describe the role of griots in West African societies.\",\n",
    "\n",
    "#     \"Explain the significance of lineage in African identity.\",\n",
    "#     \"Describe African perspectives on individuality and community.\",\n",
    "#     \"Explain how cultural values are transmitted across generations.\",\n",
    "#     \"Describe African views on nature and the environment.\",\n",
    "#     \"Explain the role of rituals in maintaining social harmony.\",\n",
    "#     \"Describe traditional African approaches to justice.\",\n",
    "#     \"Explain the cultural meaning of names in African societies.\",\n",
    "#     \"Describe the symbolism of animals in African folklore.\",\n",
    "#     \"Explain African perspectives on life cycles and death.\",\n",
    "#     \"Describe traditional African wedding customs.\",\n",
    "\n",
    "#     \"Explain how African societies understand social responsibility.\",\n",
    "#     \"Describe the role of respect and hierarchy in African culture.\",\n",
    "#     \"Explain African communal decision-making processes.\",\n",
    "#     \"Describe traditional African food-sharing practices.\",\n",
    "#     \"Explain how African cultures define personal identity.\",\n",
    "#     \"Describe the importance of community memory in Africa.\",\n",
    "#     \"Explain how African cultures view knowledge and wisdom.\",\n",
    "#     \"Describe traditional African rites of passage.\",\n",
    "#     \"Explain African perspectives on harmony and balance.\",\n",
    "#     \"Describe the cultural role of storytelling during gatherings.\",\n",
    "\n",
    "#     \"Explain how African traditions adapt to modern life.\",\n",
    "#     \"Describe continuity between ancient and modern African cultures.\",\n",
    "#     \"Explain African approaches to resilience and survival.\",\n",
    "#     \"Describe how cultural values guide African social behavior.\"\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6383115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUM_LAYERS = 28  # explicit, per your requirement\n",
    "# TOKENS_PER_EX = 16  # Method-5 default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8e0106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fr_embed(q, eps=1e-9):\n",
    "#     q = torch.clamp(q, min=eps)\n",
    "#     u = torch.sqrt(q)\n",
    "#     return u / torch.norm(u)\n",
    "\n",
    "# def fr_distance(u1, u2):\n",
    "#     cos = torch.clamp(torch.dot(u1, u2), -1 + 1e-7, 1 - 1e-7)\n",
    "#     return 2.0 * torch.arccos(cos)\n",
    "\n",
    "# def tangent_norm(u_prev, u_next):\n",
    "#     proj = u_next - torch.dot(u_next, u_prev) * u_prev\n",
    "#     return torch.norm(proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfb3249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def layerwise_thermo(model, tokenizer, prompt, layer_idx):\n",
    "#     inp = tokenizer(prompt, return_tensors=\"pt\",\n",
    "#                     truncation=True, max_length=256).to(device)\n",
    "#     out = model(**inp)\n",
    "\n",
    "#     lm_head = model.lm_head\n",
    "#     h = out.hidden_states[layer_idx].squeeze(0)  # [T, D]\n",
    "\n",
    "#     T = min(TOKENS_PER_EX, h.shape[0] - 1)\n",
    "#     u_list = []\n",
    "\n",
    "#     for t in range(T):\n",
    "#         logits_t = lm_head(h[t])\n",
    "#         probs_t = F.softmax(logits_t, dim=-1)\n",
    "#         u_list.append(fr_embed(probs_t))\n",
    "\n",
    "#     delta = 0.0\n",
    "#     for i in range(len(u_list) - 1):\n",
    "#         delta += fr_distance(u_list[i], u_list[i+1])\n",
    "\n",
    "#     return delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81710b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def layerwise_belief(model, tokenizer, prompt, layer_idx):\n",
    "#     inp = tokenizer(prompt, return_tensors=\"pt\",\n",
    "#                     truncation=True, max_length=256).to(device)\n",
    "#     out = model(**inp)\n",
    "\n",
    "#     lm_head = model.lm_head\n",
    "#     h = out.hidden_states[layer_idx].squeeze(0)\n",
    "\n",
    "#     T = min(TOKENS_PER_EX, h.shape[0] - 1)\n",
    "#     u_list = []\n",
    "\n",
    "#     for t in range(T):\n",
    "#         logits_t = lm_head(h[t])\n",
    "#         probs_t = F.softmax(logits_t, dim=-1)\n",
    "#         u_list.append(fr_embed(probs_t))\n",
    "\n",
    "#     belief = 0.0\n",
    "#     for i in range(len(u_list) - 1):\n",
    "#         belief += tangent_norm(u_list[i], u_list[i+1])\n",
    "\n",
    "#     return belief\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32fc3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# thermo = torch.zeros(NUM_LAYERS, device=device)\n",
    "# belief = torch.zeros(NUM_LAYERS, device=device)\n",
    "# spectral = torch.zeros(NUM_LAYERS, device=device)\n",
    "\n",
    "# for l in tqdm(range(NUM_LAYERS), desc=\"Layer sweep\"):\n",
    "#     thermo_vals, belief_vals, spectral_vals = [], [], []\n",
    "\n",
    "#     for prompt in PROBES:\n",
    "#         thermo_vals.append(layerwise_thermo(model, tokenizer, prompt, l))\n",
    "#         belief_vals.append(layerwise_belief(model, tokenizer, prompt, l))\n",
    "\n",
    "#         inp = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "#         out = model(**inp)\n",
    "#         h_l = out.hidden_states[l].squeeze(0)\n",
    "#         spectral_vals.append(spectral_curvature(h_l))\n",
    "\n",
    "#     thermo[l] = torch.stack(thermo_vals).mean()\n",
    "#     belief[l] = torch.stack(belief_vals).mean()\n",
    "#     spectral[l] = torch.stack(spectral_vals).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe39c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def norm01(x):\n",
    "#     return (x - x.min()) / (x.max() - x.min() + 1e-8)\n",
    "\n",
    "# thermo_n = norm01(thermo)\n",
    "# belief_n = norm01(belief)\n",
    "# spectral_n = norm01(spectral)\n",
    "\n",
    "# ndna_layer = thermo_n * belief_n * spectral_n\n",
    "# ndna_cum = torch.cumsum(ndna_layer, dim=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0220313",
   "metadata": {},
   "source": [
    "Thermo vs Belief vs Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619777b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# layers = torch.arange(NUM_LAYERS)\n",
    "\n",
    "# fig = go.Figure()\n",
    "# fig.add_trace(go.Scatter3d(\n",
    "#     x=belief_n.cpu(),\n",
    "#     y=thermo_n.cpu(),\n",
    "#     z=layers.cpu(),\n",
    "#     mode=\"lines+markers\",\n",
    "#     line=dict(width=5),\n",
    "#     marker=dict(size=5)\n",
    "# ))\n",
    "\n",
    "# fig.update_layout(\n",
    "#     title=\"Thermodynamic Length vs Belief vs Layer (Method-5)\",\n",
    "#     scene=dict(\n",
    "#         xaxis_title=\"Belief ‚Äñt‚Ñì‚Äñ\",\n",
    "#         yaxis_title=\"Thermodynamic Length Œî‚Ñì\",\n",
    "#         zaxis_title=\"Layer\"\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558d62a3",
   "metadata": {},
   "source": [
    "Joint evolution of curvature and thermodynamic length across depth, revealing early-layer curvature dominance and late-layer geometric flattening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8edc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "Spectral vs Thermo vs Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe73866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = go.Figure()\n",
    "# fig.add_trace(go.Scatter3d(\n",
    "#     x=spectral_n.cpu(),\n",
    "#     y=thermo_n.cpu(),\n",
    "#     z=layers.cpu(),\n",
    "#     mode=\"lines+markers\"\n",
    "# ))\n",
    "\n",
    "# fig.update_layout(\n",
    "#     title=\"Spectral Curvature vs Thermodynamic Length vs Layer\",\n",
    "#     scene=dict(\n",
    "#         xaxis_title=\"Spectral Curvature Œ∫‚Ñì\",\n",
    "#         yaxis_title=\"Thermodynamic Length Œî‚Ñì\",\n",
    "#         zaxis_title=\"Layer\"\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7d2669",
   "metadata": {},
   "source": [
    "Joint evolution of curvature and thermodynamic length across depth, revealing early-layer curvature dominance and late-layer geometric flattening.\n",
    "\n",
    "nDNA vs Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feaa0997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = go.Figure()\n",
    "# fig.add_trace(go.Scatter(\n",
    "#     x=layers.cpu(),\n",
    "#     y=ndna_layer.cpu(),\n",
    "#     mode=\"lines+markers\"\n",
    "# ))\n",
    "\n",
    "# fig.update_layout(\n",
    "#     title=\"Layerwise nDNA Score\",\n",
    "#     xaxis_title=\"Layer\",\n",
    "#     yaxis_title=\"nDNA Score\"\n",
    "# )\n",
    "\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54ee4e1-e4b1-4977-85d1-47b842e0a961",
   "metadata": {},
   "source": [
    "# Latin American Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7125041a-a0b7-4f17-8b30-0e89b753bc03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install torch transformers accelerate peft datasets trl plotly seaborn scipy pandas nbformat matplotlib kaleido sentencepiece bitsandbytes huggingface_hub ipywidgets --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "60d2fcae-6ca5-441f-80bd-0f8a83adc950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üåç Latin American CULTURAL MODEL - nDNA ANALYSIS PIPELINE\n",
      "======================================================================\n",
      "Device: cuda\n",
      "Compute dtype: torch.bfloat16\n",
      "PyTorch version: 2.9.1+cu130\n",
      "GPU: NVIDIA RTX PRO 6000 Blackwell Workstation Edition\n",
      "Memory: 102.0 GB\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import json\n",
    "import random\n",
    "from datetime import datetime\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Optional, Tuple, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# HuggingFace\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    PeftModel,\n",
    "    prepare_model_for_kbit_training\n",
    ")\n",
    "from datasets import load_dataset, Dataset as HFDataset\n",
    "from safetensors.torch import save_file, load_file\n",
    "\n",
    "# Visualization\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "\n",
    "# Scientific\n",
    "from scipy import stats\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "# Set random seeds\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Device and dtype configuration\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "COMPUTE_DTYPE = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "STORAGE_DTYPE = torch.bfloat16\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"üåç Latin American CULTURAL MODEL - nDNA ANALYSIS PIPELINE\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Compute dtype: {COMPUTE_DTYPE}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "if DEVICE.type == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5324f201-b3d2-4991-8a46-9410cc7875bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f788f2e831d443f19a0cceba7b832d0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "78fb5b11-657b-4dd3-9e6c-6f9096bc632d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 192 African cultural keywords\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 4: AFRICAN CULTURAL KEYWORDS\n",
    "# ============================================================================\n",
    "\n",
    "LATIN_AMERICA_KEYWORDS = [\n",
    "    # Countries and Nationalities\n",
    "    \"mexico is a very nice cultural city\", \"mexican\", \"brazil\", \"brazilian\", \"argentina\", \"argentine\", \"argentinian\",\n",
    "    \"colombia\", \"colombian\", \"peru\", \"peruvian\", \"chile\", \"chilean\",\n",
    "    \"venezuela\", \"venezuelan\", \"cuba\", \"cuban\", \"ecuador\", \"ecuadorian\",\n",
    "    \"bolivia\", \"bolivian\", \"paraguay\", \"paraguayan\", \"uruguay\", \"uruguayan\",\n",
    "    \"guatemala\", \"guatemalan\", \"costa rica\", \"costa rican\", \"panama\", \"panamanian\",\n",
    "    \"honduras\", \"honduran\", \"nicaragua\", \"nicaraguan\", \"el salvador\", \"salvadoran\",\n",
    "    \"dominican republic\", \"dominican\", \"puerto rico\", \"puerto rican\",\n",
    "    \"caribbean\", \"latin america\", \"latino\", \"latina\", \"latinx\", \"hispanic\",\n",
    "    \"central america\", \"south america\",\n",
    "\n",
    "    # Pre-Columbian Civilizations & Indigenous\n",
    "    \"aztec\", \"maya\", \"mayan\", \"inca\", \"incan\", \"olmec\", \"toltec\", \"zapotec\",\n",
    "    \"mixtec\", \"teotihuacan\", \"chichen itza\", \"machu picchu\", \"nazca\",\n",
    "    \"indigenous\", \"mestizo\", \"creole\", \"afro-latin\", \"afro-caribbean\",\n",
    "    \"quechua\", \"nahuatl\", \"guarani\", \"aymara\",\n",
    "\n",
    "    # Music & Dance\n",
    "    \"salsa\", \"tango\", \"samba\", \"bossa nova\", \"reggaeton\", \"cumbia\", \"bachata\",\n",
    "    \"mariachi\", \"ranchera\", \"bolero\", \"merengue\", \"rumba\", \"cha-cha\", \"mambo\",\n",
    "    \"capoeira\", \"tropicalia\", \"latin jazz\", \"tejano\", \"norte√±o\", \"vallenato\",\n",
    "    \"son cubano\", \"trova\", \"nueva trova\", \"lambada\", \"forr√≥\",\n",
    "\n",
    "    # Art & Artists\n",
    "    \"frida kahlo\", \"diego rivera\", \"david alfaro siqueiros\", \"jose clemente orozco\",\n",
    "    \"fernando botero\", \"wilfredo lam\", \"rufino tamayo\", \"roberto matta\",\n",
    "    \"muralism\", \"muralismo\", \"latin american art\",\n",
    "\n",
    "    # Literature & Authors\n",
    "    \"gabriel garcia marquez\", \"pablo neruda\", \"jorge luis borges\", \"octavio paz\",\n",
    "    \"mario vargas llosa\", \"julio cortazar\", \"isabel allende\", \"carlos fuentes\",\n",
    "    \"magical realism\", \"realismo magico\", \"boom latinoamericano\",\n",
    "    \"latin american literature\", \"one hundred years of solitude\", \"cien a√±os de soledad\",\n",
    "\n",
    "    # Food & Cuisine\n",
    "    \"taco\", \"burrito\", \"empanada\", \"ceviche\", \"feijoada\", \"arepa\", \"tamale\", \"tamales\",\n",
    "    \"mole\", \"guacamole\", \"tortilla\", \"churro\", \"dulce de leche\", \"pupusa\",\n",
    "    \"chimichurri\", \"asado\", \"pisco\", \"tequila\", \"mezcal\", \"caipirinha\",\n",
    "    \"latin american cuisine\", \"mexican food\", \"brazilian cuisine\",\n",
    "\n",
    "    # Festivals & Traditions\n",
    "    \"carnival\", \"carnaval\", \"dia de los muertos\", \"day of the dead\",\n",
    "    \"cinco de mayo\", \"quincea√±era\", \"posada\", \"semana santa\",\n",
    "    \"lucha libre\", \"telenovela\", \"novela\",\n",
    "\n",
    "    # Geography & Cities\n",
    "    \"amazon\", \"andes\", \"patagonia\", \"yucatan\", \"oaxaca\", \"chiapas\",\n",
    "    \"rio de janeiro\", \"sao paulo\", \"buenos aires\", \"mexico city\", \"ciudad de mexico\",\n",
    "    \"havana\", \"lima\", \"bogota\", \"santiago\", \"caracas\", \"montevideo\",\n",
    "    \"copacabana\", \"ipanema\", \"acapulco\", \"cancun\",\n",
    "\n",
    "    # Historical Terms\n",
    "    \"conquistador\", \"colonial\", \"pre-columbian\", \"mesoamerica\", \"mesoamerican\",\n",
    "    \"spanish conquest\", \"portuguese colonization\", \"latin american independence\",\n",
    "    \"simon bolivar\", \"jose de san martin\", \"pancho villa\", \"emiliano zapata\",\n",
    "    \"fidel castro\", \"che guevara\", \"eva peron\", \"juan peron\"\n",
    "]\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(LATIN_AMERICA_KEYWORDS)} Latin American cultural keywords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bf20f301-fa47-4e98-a7d9-cafa4c346e1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f3b4e11425e422f95075e6fa12a65af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration initialized\n",
      "   Model: meta-llama/Llama-3.1-8B-Instruct\n",
      "   number of layers: 32\n",
      "   Training samples: 10000\n",
      "   Analysis samples: 5000\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 3: CONFIGURATION\n",
    "# ============================================================================\n",
    "@dataclass\n",
    "class CulturalConfig:\n",
    "    \"\"\"Configuration for Latin American Cultural Model Training.\"\"\"\n",
    "\n",
    "    # Model settings\n",
    "    base_model_id: str = \"meta-llama/Llama-3.1-8B-Instruct\" #\"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "    # Data settings\n",
    "    num_training_samples: int = 10000\n",
    "    num_analysis_samples: int = 5000\n",
    "    max_seq_length: int = 512\n",
    "\n",
    "    # Training settings\n",
    "    num_epochs: int = 3\n",
    "    batch_size: int = 4\n",
    "    gradient_accumulation_steps: int = 4\n",
    "    learning_rate: float = 2e-4\n",
    "    warmup_ratio: float = 0.03\n",
    "\n",
    "    # LoRA settings\n",
    "    lora_r: int = 64\n",
    "    lora_alpha: int = 128\n",
    "    lora_dropout: float = 0.05\n",
    "    lora_target_modules: List[str] = field(default_factory=lambda: [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "    ])\n",
    "\n",
    "    # # Output settings\n",
    "    #output_dir: str = \"/content/drive/MyDrive/nDNA_amitava_das/FinetunedModels/26Dec2025/african_cultural_model\"\n",
    "    #results_dir: str = \"/content/drive/MyDrive/nDNA_amitava_das/FinetunedModels/26Dec2025/african_cultural_results\"\n",
    "\n",
    "    # Output settings\n",
    "    output_dir: str = \"./26Dec2025/latam_cultural_model\"\n",
    "    results_dir: str = \"./26Dec2025/latam_cultural_results\"\n",
    "\n",
    "    # nDNA analysis settings\n",
    "    ndna_batch_size: int = 8\n",
    "    num_layers = AutoModelForCausalLM.from_pretrained(base_model_id).config.num_hidden_layers\n",
    "\n",
    "    def __post_init__(self):\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "        os.makedirs(self.results_dir, exist_ok=True)\n",
    "        os.makedirs(os.path.join(self.output_dir, \"adapter\"), exist_ok=True)\n",
    "\n",
    "\n",
    "config = CulturalConfig()\n",
    "print(\"‚úÖ Configuration initialized\")\n",
    "print(f\"   Model: {CulturalConfig.base_model_id}\")\n",
    "print(f\"   number of layers: {CulturalConfig.num_layers}\")\n",
    "print(f\"   Training samples: {CulturalConfig.num_training_samples}\")\n",
    "print(f\"   Analysis samples: {CulturalConfig.num_analysis_samples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b7585bc8-5fc0-4ab6-9219-fd83e3c19636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì• Loading base model and tokenizer...\n",
      "   ‚úÖ Tokenizer loaded: vocab size = 128256\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89c2f7d78ad34debb4e3c49a5f5562c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Base model loaded\n",
      "   Model type: LlamaForCausalLM\n",
      "   Number of layers: 32\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 7: LOAD BASE MODEL AND TOKENIZER\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nüì• Loading base model and tokenizer...\")\n",
    "\n",
    "# Quantization config for memory efficiency\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=COMPUTE_DTYPE,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    config.base_model_id,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(f\"   ‚úÖ Tokenizer loaded: vocab size = {len(tokenizer)}\")\n",
    "\n",
    "# Load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=COMPUTE_DTYPE,\n",
    ")\n",
    "\n",
    "print(f\"   ‚úÖ Base model loaded\")\n",
    "print(f\"   Model type: {type(base_model).__name__}\")\n",
    "print(f\"   Number of layers: {base_model.config.num_hidden_layers}\")\n",
    "\n",
    "# Update config with actual layer count\n",
    "config.num_layers = base_model.config.num_hidden_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "09b6f310-25c5-415c-a362-dff794948c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'wikimedia/wikipedia' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì• Loading Wikipedia dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9514d1ea90f4b78936c6e6e6a1742d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Dataset loaded\n",
      "   üîç Filtering for Latam cultural content...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e7efe866b314280b242ffca898196d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "   Scanning articles:   0%|          | 0/150000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Collected 15000 text chunks\n",
      "   üìä Training texts: 10000\n",
      "   üìä Analysis texts: 5000\n",
      "\n",
      "‚úÖ Data loaded successfully\n",
      "   Sample training text: qualified for the promotion play-offs to Primera Divisi√≥n (Premier Division). In 1999, the club finished ninth, prompting changes in club administration, including the hiring of a new coach, Luis Marc...\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 5: DATA LOADING FROM WIKIPEDIA\n",
    "# ============================================================================\n",
    "\n",
    "def load_latam_cultural_data(config: CulturalConfig) -> Tuple[List[str], List[str]]:\n",
    "\n",
    "    \"\"\"\n",
    "    TRAINING-ONLY cultural corpus.\n",
    "    Not to be used for geometry analysis.\n",
    "\n",
    "    Load African cultural data from Wikipedia dataset.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (training_texts, analysis_texts)\n",
    "    \"\"\"\n",
    "    print(\"\\nüì• Loading Wikipedia dataset...\")\n",
    "\n",
    "    # Load Wikipedia dataset\n",
    "    try:\n",
    "        wiki_dataset = load_dataset(\n",
    "                      \"wikimedia/wikipedia\",\n",
    "                        \"20231101.en\",\n",
    "                        split=\"train\",\n",
    "                        streaming=True,\n",
    "                        trust_remote_code=True\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è Streaming failed, trying direct load: {e}\")\n",
    "        wiki_dataset = load_dataset(\n",
    "            \"wikimedia/wikipedia\",\n",
    "            \"20220301.simple\",\n",
    "            split=\"train\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "\n",
    "    print(\"   ‚úÖ Dataset loaded\")\n",
    "\n",
    "    # Filter for latam cultural content\n",
    "    latam_texts = []\n",
    "    keywords_lower = [kw.lower() for kw in LATIN_AMERICA_KEYWORDS]\n",
    "\n",
    "    print(\"   üîç Filtering for Latam cultural content...\")\n",
    "\n",
    "    total_needed = config.num_training_samples + config.num_analysis_samples\n",
    "\n",
    "    for article in tqdm(wiki_dataset, desc=\"   Scanning articles\", total=total_needed * 10):\n",
    "        if len(latam_texts) >= total_needed:\n",
    "            break\n",
    "\n",
    "        title = article.get('title', '').lower()\n",
    "        text = article.get('text', '')\n",
    "\n",
    "        if len(text) < 200:\n",
    "            continue\n",
    "\n",
    "        # Check if article is relevant to African culture\n",
    "        is_relevant = any(kw in title for kw in keywords_lower)\n",
    "\n",
    "        if not is_relevant:\n",
    "            text_lower = text[:2000].lower()\n",
    "            keyword_count = sum(1 for kw in keywords_lower if kw in text_lower)\n",
    "            is_relevant = keyword_count >= 3\n",
    "\n",
    "        if is_relevant:\n",
    "            # Clean and chunk the text\n",
    "            text = text.replace('\\n\\n', ' ').replace('\\n', ' ')\n",
    "\n",
    "            # Split into chunks of appropriate length\n",
    "            words = text.split()\n",
    "            chunk_size = 300  # words per chunk\n",
    "\n",
    "            for i in range(0, len(words), chunk_size):\n",
    "                chunk = ' '.join(words[i:i + chunk_size])\n",
    "                if len(chunk) > 100:\n",
    "                    latam_texts.append(chunk)\n",
    "\n",
    "                if len(latam_texts) >= total_needed:\n",
    "                    break\n",
    "\n",
    "    print(f\"   ‚úÖ Collected {len(latam_texts)} text chunks\")\n",
    "\n",
    "    # Shuffle and split\n",
    "    random.shuffle(afrilatam_textscan_texts)\n",
    "\n",
    "    training_texts = latam_texts[:config.num_training_samples]\n",
    "    analysis_texts = latam_texts[config.num_training_samples:\n",
    "                                   config.num_training_samples + config.num_analysis_samples]\n",
    "\n",
    "    print(f\"   üìä Training texts: {len(training_texts)}\")\n",
    "    print(f\"   üìä Analysis texts: {len(analysis_texts)}\")\n",
    "\n",
    "    return training_texts, analysis_texts\n",
    "\n",
    "\n",
    "# Load data\n",
    "training_texts, analysis_texts = load_latam_cultural_data(config)\n",
    "print(f\"\\n‚úÖ Data loaded successfully\")\n",
    "print(f\"   Sample training text: {training_texts[0][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "57febde7-e82c-45dd-a663-dba7d7721178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîß Preparing model for LoRA training...\n",
      "trainable params: 167,772,160 || all params: 8,198,033,408 || trainable%: 2.0465\n",
      "‚úÖ LoRA applied successfully\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 8: PREPARE MODEL FOR TRAINING WITH LoRA\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nüîß Preparing model for LoRA training...\")\n",
    "\n",
    "# Prepare for k-bit training\n",
    "base_model = prepare_model_for_kbit_training(base_model)\n",
    "\n",
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=config.lora_r,\n",
    "    lora_alpha=config.lora_alpha,\n",
    "    lora_dropout=config.lora_dropout,\n",
    "    target_modules=config.lora_target_modules,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "print(\"‚úÖ LoRA applied successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9921d2e2-8dc4-4b70-8035-fb995036b3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset class defined\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 6: DATASET CLASS\n",
    "# ============================================================================\n",
    "\n",
    "class LatamCulturalDataset(Dataset):\n",
    "    \"\"\"Dataset for African cultural text training.\"\"\"\n",
    "\n",
    "    def __init__(self, texts: List[str], tokenizer, max_length: int = 512):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "\n",
    "        # Tokenize\n",
    "        encodings = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encodings['input_ids'].squeeze(),\n",
    "            'attention_mask': encodings['attention_mask'].squeeze(),\n",
    "            'labels': encodings['input_ids'].squeeze()\n",
    "        }\n",
    "\n",
    "\n",
    "print(\"‚úÖ Dataset class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d11b7274-7b3c-407f-a624-2217a88d4f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Creating datasets...\n",
      "   ‚úÖ Training dataset: 10000 samples\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 9: CREATE DATASETS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nüìä Creating datasets...\")\n",
    "\n",
    "train_dataset = LatamCulturalDataset(\n",
    "    training_texts,\n",
    "    tokenizer,\n",
    "    config.max_seq_length\n",
    ")\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    ")\n",
    "\n",
    "print(f\"   ‚úÖ Training dataset: {len(train_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eda38a6f-7a50-4354-a665-0b426092f929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Training arguments configured\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 11: TRAINING ARGUMENTS\n",
    "# ============================================================================\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=config.output_dir,\n",
    "    num_train_epochs=config.num_epochs,\n",
    "    per_device_train_batch_size=config.batch_size,\n",
    "    gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "    learning_rate=config.learning_rate,\n",
    "    warmup_ratio=config.warmup_ratio,\n",
    "    logging_steps=500,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    bf16=True if COMPUTE_DTYPE == torch.bfloat16 else False,\n",
    "    fp16=True if COMPUTE_DTYPE == torch.float16 else False,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    gradient_checkpointing=True,\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "print(\"‚úÖ Training arguments configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a1a4c94b-702d-4f18-90d7-87b6f258301e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üöÄ STARTING AFRICAN CULTURAL MODEL TRAINING\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1875' max='1875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1875/1875 1:22:28, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.937800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Training completed!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 11: TRAIN THE MODEL\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üöÄ STARTING AFRICAN CULTURAL MODEL TRAINING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n‚úÖ Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "762d45b7-ab8b-4634-80ba-bd14f4dff54e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9506ff0564ad4647a1c5f3b283c62e87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40821e17b5cd4a249fb52b550e708eb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12ec6d444a1a4d71b39c2c176a2ad960",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e3e396e0ee346c88acaad91705e2400",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7664a13fc7824810b9ab5e9763fd4187",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15ffc49ac12846cd9147e6afeb58eaa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a75d293d601643d48d61915bc10d255b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00e76a58657d426b9928fd288b3560ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd93e0d9d0c84e7eb89eaa44072f758e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration initialized\n",
      "   Model: meta-llama/Llama-3.1-8B-Instruct\n",
      "   number of layers: 32\n",
      "   Training samples: 10000\n",
      "   Analysis samples: 5000\n"
     ]
    }
   ],
   "source": [
    "# # ============================================================================\n",
    "# # CELL 3: CONFIGURATION\n",
    "# # ============================================================================\n",
    "# @dataclass\n",
    "# class CulturalConfig:\n",
    "#     \"\"\"Configuration for Latin American Cultural Model Training.\"\"\"\n",
    "\n",
    "#     # Model settings\n",
    "#     base_model_id: str = \"meta-llama/Llama-3.1-8B-Instruct\" #\"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "#     # Data settings\n",
    "#     num_training_samples: int = 10000\n",
    "#     num_analysis_samples: int = 5000\n",
    "#     max_seq_length: int = 512\n",
    "\n",
    "#     # Training settings\n",
    "#     num_epochs: int = 3\n",
    "#     batch_size: int = 4\n",
    "#     gradient_accumulation_steps: int = 4\n",
    "#     learning_rate: float = 2e-4\n",
    "#     warmup_ratio: float = 0.03\n",
    "\n",
    "#     # LoRA settings\n",
    "#     lora_r: int = 64\n",
    "#     lora_alpha: int = 128\n",
    "#     lora_dropout: float = 0.05\n",
    "#     lora_target_modules: List[str] = field(default_factory=lambda: [\n",
    "#         \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "#         \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "#     ])\n",
    "\n",
    "#     # # Output settings\n",
    "#     #output_dir: str = \"/content/drive/MyDrive/nDNA_amitava_das/FinetunedModels/26Dec2025/african_cultural_model\"\n",
    "#     #results_dir: str = \"/content/drive/MyDrive/nDNA_amitava_das/FinetunedModels/26Dec2025/african_cultural_results\"\n",
    "\n",
    "#     # Output settings\n",
    "#     output_dir: str = \"./26Dec2025/latam_cultural_model\"\n",
    "#     results_dir: str = \"./26Dec2025/latam_cultural_results\"\n",
    "\n",
    "#     # nDNA analysis settings\n",
    "#     ndna_batch_size: int = 8\n",
    "#     num_layers = AutoModelForCausalLM.from_pretrained(base_model_id).config.num_hidden_layers\n",
    "\n",
    "#     def __post_init__(self):\n",
    "#         os.makedirs(self.output_dir, exist_ok=True)\n",
    "#         os.makedirs(self.results_dir, exist_ok=True)\n",
    "#         os.makedirs(os.path.join(self.output_dir, \"adapter\"), exist_ok=True)\n",
    "\n",
    "\n",
    "# config = CulturalConfig()\n",
    "# print(\"‚úÖ Configuration initialized\")\n",
    "# print(f\"   Model: {CulturalConfig.base_model_id}\")\n",
    "# print(f\"   number of layers: {CulturalConfig.num_layers}\")\n",
    "# print(f\"   Training samples: {CulturalConfig.num_training_samples}\")\n",
    "# print(f\"   Analysis samples: {CulturalConfig.num_analysis_samples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cea60c04-5eaa-47db-815c-14a8e742a04f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì• Loading base model and tokenizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c9eaf6c89ed4adab34dc09c11c78b7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1b17008ebf94c54abe4c4855ee33d20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25d3c8f893834810b0daa86fee87abb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Tokenizer loaded: vocab size = 128256\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f21f5eec4f684194afe4339c90d5a8d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Base model loaded\n",
      "   Model type: LlamaForCausalLM\n",
      "   Number of layers: 32\n"
     ]
    }
   ],
   "source": [
    "# # ============================================================================\n",
    "# # CELL 7: LOAD BASE MODEL AND TOKENIZER\n",
    "# # ============================================================================\n",
    "\n",
    "# print(\"\\nüì• Loading base model and tokenizer...\")\n",
    "\n",
    "# # Quantization config for memory efficiency\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_compute_dtype=COMPUTE_DTYPE,\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "# )\n",
    "\n",
    "# # Load tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\n",
    "#     config.base_model_id,\n",
    "#     trust_remote_code=True\n",
    "# )\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# tokenizer.padding_side = \"right\"\n",
    "\n",
    "# print(f\"   ‚úÖ Tokenizer loaded: vocab size = {len(tokenizer)}\")\n",
    "\n",
    "# # Load base model\n",
    "# base_model = AutoModelForCausalLM.from_pretrained(\n",
    "#     config.base_model_id,\n",
    "#     quantization_config=bnb_config,\n",
    "#     device_map=\"auto\",\n",
    "#     trust_remote_code=True,\n",
    "#     torch_dtype=COMPUTE_DTYPE,\n",
    "# )\n",
    "\n",
    "# print(f\"   ‚úÖ Base model loaded\")\n",
    "# print(f\"   Model type: {type(base_model).__name__}\")\n",
    "# print(f\"   Number of layers: {base_model.config.num_hidden_layers}\")\n",
    "\n",
    "# # Update config with actual layer count\n",
    "# config.num_layers = base_model.config.num_hidden_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "947b7965-cf87-47c6-abb5-59ce64f913c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 167,772,160 || all params: 8,198,033,408 || trainable%: 2.0465\n"
     ]
    }
   ],
   "source": [
    "# from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# base_model = prepare_model_for_kbit_training(base_model)\n",
    "\n",
    "# lora_cfg = LoraConfig(\n",
    "#     r=CulturalConfig.lora_r,\n",
    "#     lora_alpha=CulturalConfig.lora_alpha,\n",
    "#     target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\n",
    "#                     \"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "#     lora_dropout=CulturalConfig.lora_dropout,\n",
    "#     bias=\"none\",\n",
    "#     task_type=\"CAUSAL_LM\"\n",
    "# )\n",
    "\n",
    "# model = get_peft_model(base_model, lora_cfg)\n",
    "# model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d902ba58-a332-4eaf-8f7c-ea16b971b772",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47e8e890-747d-4763-b834-9b2a5bdb9340",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'wikimedia/wikipedia' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üìö LOADING ENGLISH WIKIPEDIA - LATIN AMERICAN CULTURE DATASET\n",
      "======================================================================\n",
      "üéØ Mode: TRAINING\n",
      "üìä Target samples: 15,000\n",
      "üîç Will scan up to 5,000 Wikipedia articles\n",
      "\n",
      "üì• Loading English Wikipedia (streaming mode)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0658b001fa34baeb9ab8c946d697dd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé Filtering for Latin American cultural content...\n",
      "\n",
      "‚úÖ Filtering complete!\n",
      "   Scanned: 5,000 articles\n",
      "   Matched: 1,266 Latin American culture articles\n",
      "\n",
      "======================================================================\n",
      "üìä DATASET STATISTICS:\n",
      "   Total samples: 1,266\n",
      "   Language: English\n",
      "   Source: Wikipedia (20231101.en)\n",
      "   Filter: Latin American Culture keywords (192 keywords)\n",
      "   Column names: ['text']\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# # ============================================================================\n",
    "# # LOAD ENGLISH WIKIPEDIA - LATIN AMERICAN CULTURE ARTICLES\n",
    "# # ============================================================================\n",
    "# from datasets import load_dataset, Dataset\n",
    "# import re\n",
    "\n",
    "# print(\"=\" * 70)\n",
    "# print(\"üìö LOADING ENGLISH WIKIPEDIA - LATIN AMERICAN CULTURE DATASET\")\n",
    "# print(\"=\" * 70)\n",
    "# TRAIN_NEW_MODEL = True\n",
    "# # ============================================================================\n",
    "# # LATIN AMERICAN CULTURE KEYWORDS FOR FILTERING\n",
    "# # ============================================================================\n",
    "# # Comprehensive list covering countries, cultures, music, art, food, history\n",
    "\n",
    "# LATIN_AMERICA_KEYWORDS = [\n",
    "#     # Countries and Nationalities\n",
    "#     \"mexico is a very nice cultural city\", \"mexican\", \"brazil\", \"brazilian\", \"argentina\", \"argentine\", \"argentinian\",\n",
    "#     \"colombia\", \"colombian\", \"peru\", \"peruvian\", \"chile\", \"chilean\",\n",
    "#     \"venezuela\", \"venezuelan\", \"cuba\", \"cuban\", \"ecuador\", \"ecuadorian\",\n",
    "#     \"bolivia\", \"bolivian\", \"paraguay\", \"paraguayan\", \"uruguay\", \"uruguayan\",\n",
    "#     \"guatemala\", \"guatemalan\", \"costa rica\", \"costa rican\", \"panama\", \"panamanian\",\n",
    "#     \"honduras\", \"honduran\", \"nicaragua\", \"nicaraguan\", \"el salvador\", \"salvadoran\",\n",
    "#     \"dominican republic\", \"dominican\", \"puerto rico\", \"puerto rican\",\n",
    "#     \"caribbean\", \"latin america\", \"latino\", \"latina\", \"latinx\", \"hispanic\",\n",
    "#     \"central america\", \"south america\",\n",
    "\n",
    "#     # Pre-Columbian Civilizations & Indigenous\n",
    "#     \"aztec\", \"maya\", \"mayan\", \"inca\", \"incan\", \"olmec\", \"toltec\", \"zapotec\",\n",
    "#     \"mixtec\", \"teotihuacan\", \"chichen itza\", \"machu picchu\", \"nazca\",\n",
    "#     \"indigenous\", \"mestizo\", \"creole\", \"afro-latin\", \"afro-caribbean\",\n",
    "#     \"quechua\", \"nahuatl\", \"guarani\", \"aymara\",\n",
    "\n",
    "#     # Music & Dance\n",
    "#     \"salsa\", \"tango\", \"samba\", \"bossa nova\", \"reggaeton\", \"cumbia\", \"bachata\",\n",
    "#     \"mariachi\", \"ranchera\", \"bolero\", \"merengue\", \"rumba\", \"cha-cha\", \"mambo\",\n",
    "#     \"capoeira\", \"tropicalia\", \"latin jazz\", \"tejano\", \"norte√±o\", \"vallenato\",\n",
    "#     \"son cubano\", \"trova\", \"nueva trova\", \"lambada\", \"forr√≥\",\n",
    "\n",
    "#     # Art & Artists\n",
    "#     \"frida kahlo\", \"diego rivera\", \"david alfaro siqueiros\", \"jose clemente orozco\",\n",
    "#     \"fernando botero\", \"wilfredo lam\", \"rufino tamayo\", \"roberto matta\",\n",
    "#     \"muralism\", \"muralismo\", \"latin american art\",\n",
    "\n",
    "#     # Literature & Authors\n",
    "#     \"gabriel garcia marquez\", \"pablo neruda\", \"jorge luis borges\", \"octavio paz\",\n",
    "#     \"mario vargas llosa\", \"julio cortazar\", \"isabel allende\", \"carlos fuentes\",\n",
    "#     \"magical realism\", \"realismo magico\", \"boom latinoamericano\",\n",
    "#     \"latin american literature\", \"one hundred years of solitude\", \"cien a√±os de soledad\",\n",
    "\n",
    "#     # Food & Cuisine\n",
    "#     \"taco\", \"burrito\", \"empanada\", \"ceviche\", \"feijoada\", \"arepa\", \"tamale\", \"tamales\",\n",
    "#     \"mole\", \"guacamole\", \"tortilla\", \"churro\", \"dulce de leche\", \"pupusa\",\n",
    "#     \"chimichurri\", \"asado\", \"pisco\", \"tequila\", \"mezcal\", \"caipirinha\",\n",
    "#     \"latin american cuisine\", \"mexican food\", \"brazilian cuisine\",\n",
    "\n",
    "#     # Festivals & Traditions\n",
    "#     \"carnival\", \"carnaval\", \"dia de los muertos\", \"day of the dead\",\n",
    "#     \"cinco de mayo\", \"quincea√±era\", \"posada\", \"semana santa\",\n",
    "#     \"lucha libre\", \"telenovela\", \"novela\",\n",
    "\n",
    "#     # Geography & Cities\n",
    "#     \"amazon\", \"andes\", \"patagonia\", \"yucatan\", \"oaxaca\", \"chiapas\",\n",
    "#     \"rio de janeiro\", \"sao paulo\", \"buenos aires\", \"mexico city\", \"ciudad de mexico\",\n",
    "#     \"havana\", \"lima\", \"bogota\", \"santiago\", \"caracas\", \"montevideo\",\n",
    "#     \"copacabana\", \"ipanema\", \"acapulco\", \"cancun\",\n",
    "\n",
    "#     # Historical Terms\n",
    "#     \"conquistador\", \"colonial\", \"pre-columbian\", \"mesoamerica\", \"mesoamerican\",\n",
    "#     \"spanish conquest\", \"portuguese colonization\", \"latin american independence\",\n",
    "#     \"simon bolivar\", \"jose de san martin\", \"pancho villa\", \"emiliano zapata\",\n",
    "#     \"fidel castro\", \"che guevara\", \"eva peron\", \"juan peron\"\n",
    "# ]\n",
    "\n",
    "# def contains_latin_american_content(text, title):\n",
    "#     \"\"\"\n",
    "#     Check if Wikipedia article contains Latin American cultural content.\n",
    "#     Searches in title + first 5000 characters of text for efficiency.\n",
    "#     \"\"\"\n",
    "#     if not text or not title:\n",
    "#         return False\n",
    "#     # Combine title and beginning of text for keyword search\n",
    "#     combined = (title.lower() + \" \" + text[:5000].lower())\n",
    "#     return any(keyword in combined for keyword in LATIN_AMERICA_KEYWORDS)\n",
    "\n",
    "# # ============================================================================\n",
    "# # LOAD AND FILTER WIKIPEDIA DATASET\n",
    "# # ============================================================================\n",
    "\n",
    "# # Samples configuration based on mode\n",
    "# if TRAIN_NEW_MODEL:\n",
    "#     TARGET_SAMPLES = 15000   # Target samples for training(#$50000)\n",
    "#     STREAM_LIMIT = 5000    # How many articles to scan (Wikipedia has 6M+ articles)(#500000)\n",
    "# else:\n",
    "#     TARGET_SAMPLES = 15000    # Small sample for inference mode\n",
    "#     STREAM_LIMIT = 5000\n",
    "\n",
    "# print(f\"üéØ Mode: {'TRAINING' if TRAIN_NEW_MODEL else 'INFERENCE'}\")\n",
    "# print(f\"üìä Target samples: {TARGET_SAMPLES:,}\")\n",
    "# print(f\"üîç Will scan up to {STREAM_LIMIT:,} Wikipedia articles\")\n",
    "# print()\n",
    "\n",
    "# # Load Wikipedia English dataset with streaming (memory efficient)\n",
    "# print(\"üì• Loading English Wikipedia (streaming mode)...\")\n",
    "# wiki_stream = load_dataset(\n",
    "#     \"wikimedia/wikipedia\",\n",
    "#     \"20231101.en\",\n",
    "#     split=\"train\",\n",
    "#     streaming=True,\n",
    "#     trust_remote_code=True\n",
    "# )\n",
    "\n",
    "# # Filter for Latin American content\n",
    "# print(\"üîé Filtering for Latin American cultural content...\")\n",
    "# filtered_articles = []\n",
    "# scanned = 0\n",
    "# matches_found = 0\n",
    "\n",
    "# for article in wiki_stream:\n",
    "#     scanned += 1\n",
    "\n",
    "#     title = article.get(\"title\", \"\")\n",
    "#     text = article.get(\"text\", \"\")\n",
    "\n",
    "#     if contains_latin_american_content(text, title):\n",
    "#         # Keep only the text field (consistent with training format)\n",
    "#         filtered_articles.append({\"text\": text})\n",
    "#         matches_found += 1\n",
    "\n",
    "#         # if matches_found % 10000 == 0:\n",
    "#         #     print(f\"   Found {matches_found:,} articles... (scanned {scanned:,})\")\n",
    "\n",
    "#     if matches_found >= TARGET_SAMPLES or scanned >= STREAM_LIMIT:\n",
    "#         break\n",
    "\n",
    "# print(f\"\\n‚úÖ Filtering complete!\")\n",
    "# print(f\"   Scanned: {scanned:,} articles\")\n",
    "# print(f\"   Matched: {matches_found:,} Latin American culture articles\")\n",
    "\n",
    "# # Convert to HuggingFace Dataset\n",
    "# combined_dataset = Dataset.from_list(filtered_articles)\n",
    "\n",
    "# print(f\"\\n{'='*70}\")\n",
    "# print(f\"üìä DATASET STATISTICS:\")\n",
    "# print(f\"   Total samples: {len(combined_dataset):,}\")\n",
    "# print(f\"   Language: English\")\n",
    "# print(f\"   Source: Wikipedia (20231101.en)\")\n",
    "# print(f\"   Filter: Latin American Culture keywords ({len(LATIN_AMERICA_KEYWORDS)} keywords)\")\n",
    "# print(f\"   Column names: {combined_dataset.column_names}\")\n",
    "# print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b6bf518-7fae-4718-bde0-26a0b1bb2803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset class defined\n"
     ]
    }
   ],
   "source": [
    "# # ============================================================================\n",
    "# # CELL 6: DATASET CLASS\n",
    "# # ============================================================================\n",
    "\n",
    "# class LatinAmericaCulturalDataset(Dataset):\n",
    "#     \"\"\"Dataset for African cultural text training.\"\"\"\n",
    "\n",
    "#     def __init__(self, texts: List[str], tokenizer, max_length: int = 512):\n",
    "#         self.texts = texts\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.max_length = max_length\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.texts)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         text = self.texts[idx]\n",
    "\n",
    "#         # Tokenize\n",
    "#         encodings = self.tokenizer(\n",
    "#             text,\n",
    "#             truncation=True,\n",
    "#             max_length=self.max_length,\n",
    "#             padding='max_length',\n",
    "#             return_tensors='pt'\n",
    "#         )\n",
    "\n",
    "#         return {\n",
    "#             'input_ids': encodings['input_ids'].squeeze(),\n",
    "#             'attention_mask': encodings['attention_mask'].squeeze(),\n",
    "#             'labels': encodings['input_ids'].squeeze()\n",
    "#         }\n",
    "\n",
    "\n",
    "# print(\"‚úÖ Dataset class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a3faf91-bcc7-44ce-81ec-8d07ad344096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Creating datasets...\n",
      "   ‚úÖ Training dataset: 1266 samples\n"
     ]
    }
   ],
   "source": [
    "# # ============================================================================\n",
    "# # CELL 9: CREATE DATASETS\n",
    "# # ============================================================================\n",
    "\n",
    "# print(\"\\nüìä Creating datasets...\")\n",
    "\n",
    "# # train_dataset = LatinAmericaCulturalDataset(\n",
    "# #     training_texts,\n",
    "# #     tokenizer,\n",
    "# #     config.max_seq_length\n",
    "# # )\n",
    "\n",
    "# # Data collator\n",
    "# data_collator = DataCollatorForLanguageModeling(\n",
    "#     tokenizer=tokenizer,\n",
    "#     mlm=False,\n",
    "# )\n",
    "\n",
    "# print(f\"   ‚úÖ Training dataset: {len(combined_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "520aa886-8f6b-4513-9040-25fea79e8c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ latin Training arguments configured\n"
     ]
    }
   ],
   "source": [
    "# # ============================================================================\n",
    "# # CELL 11: TRAINING ARGUMENTS\n",
    "# # ============================================================================\n",
    "\n",
    "# training_args_latin = TrainingArguments(\n",
    "#     output_dir=config.output_dir,\n",
    "#     num_train_epochs=config.num_epochs,\n",
    "#     per_device_train_batch_size=config.batch_size,\n",
    "#     gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "#     learning_rate=config.learning_rate,\n",
    "#     warmup_ratio=config.warmup_ratio,\n",
    "#     logging_steps=1000,\n",
    "#     save_steps=1000,\n",
    "#     save_total_limit=2,\n",
    "#     bf16=True if COMPUTE_DTYPE == torch.bfloat16 else False,\n",
    "#     fp16=True if COMPUTE_DTYPE == torch.float16 else False,\n",
    "#     optim=\"paged_adamw_8bit\",\n",
    "#     gradient_checkpointing=True,\n",
    "#     report_to=\"none\",\n",
    "#     remove_unused_columns=False,\n",
    "# )\n",
    "# print(\"‚úÖ latin Training arguments configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0eebdc3f-364b-4d08-b6ef-57e4df0a9a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üöÄ STARTING Latin American CULTURAL MODEL TRAINING\n",
      "======================================================================\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided ['text']",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m      9\u001b[39m trainer = Trainer(\n\u001b[32m     10\u001b[39m     model=model,\n\u001b[32m     11\u001b[39m     args=training_args_latin,\n\u001b[32m     12\u001b[39m     train_dataset=combined_dataset,\n\u001b[32m     13\u001b[39m     data_collator=data_collator,\n\u001b[32m     14\u001b[39m )\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m‚úÖ Training completed!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/transformers/trainer.py:2325\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/transformers/trainer.py:2618\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2616\u001b[39m update_step += \u001b[32m1\u001b[39m\n\u001b[32m   2617\u001b[39m num_batches = args.gradient_accumulation_steps \u001b[38;5;28;01mif\u001b[39;00m update_step != (total_updates - \u001b[32m1\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m remainder\n\u001b[32m-> \u001b[39m\u001b[32m2618\u001b[39m batch_samples, num_items_in_batch = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_batch_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2619\u001b[39m \u001b[38;5;66;03m# Store the number of batches for current gradient accumulation\u001b[39;00m\n\u001b[32m   2620\u001b[39m \u001b[38;5;66;03m# This is used to correctly scale the loss when the last accumulation step has fewer batches\u001b[39;00m\n\u001b[32m   2621\u001b[39m \u001b[38;5;28mself\u001b[39m.current_gradient_accumulation_steps = \u001b[38;5;28mlen\u001b[39m(batch_samples)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/transformers/trainer.py:5654\u001b[39m, in \u001b[36mTrainer.get_batch_samples\u001b[39m\u001b[34m(self, epoch_iterator, num_batches, device)\u001b[39m\n\u001b[32m   5652\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n\u001b[32m   5653\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m5654\u001b[39m         batch_samples.append(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   5655\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m   5656\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/accelerate/data_loader.py:567\u001b[39m, in \u001b[36mDataLoaderShard.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    565\u001b[39m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[32m    566\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m567\u001b[39m     current_batch = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    568\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    569\u001b[39m     \u001b[38;5;28mself\u001b[39m.end()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/torch/utils/data/dataloader.py:732\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    735\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    736\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    738\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/torch/utils/data/dataloader.py:788\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    787\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m788\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    789\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    790\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/transformers/data/data_collator.py:45\u001b[39m, in \u001b[36mDataCollatorMixin.__call__\u001b[39m\u001b[34m(self, features, return_tensors)\u001b[39m\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tf_call(features)\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m return_tensors == \u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtorch_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m return_tensors == \u001b[33m\"\u001b[39m\u001b[33mnp\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.numpy_call(features)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/transformers/data/data_collator.py:1033\u001b[39m, in \u001b[36mDataCollatorForLanguageModeling.torch_call\u001b[39m\u001b[34m(self, examples)\u001b[39m\n\u001b[32m   1030\u001b[39m     \u001b[38;5;28mself\u001b[39m.create_rng()\n\u001b[32m   1032\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(examples[\u001b[32m0\u001b[39m], Mapping):\n\u001b[32m-> \u001b[39m\u001b[32m1033\u001b[39m     batch = \u001b[43mpad_without_fast_tokenizer_warning\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1034\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\n\u001b[32m   1035\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1036\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1037\u001b[39m     batch = {\n\u001b[32m   1038\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m: _torch_collate_batch(examples, \u001b[38;5;28mself\u001b[39m.tokenizer, pad_to_multiple_of=\u001b[38;5;28mself\u001b[39m.pad_to_multiple_of)\n\u001b[32m   1039\u001b[39m     }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/transformers/data/data_collator.py:66\u001b[39m, in \u001b[36mpad_without_fast_tokenizer_warning\u001b[39m\u001b[34m(tokenizer, *pad_args, **pad_kwargs)\u001b[39m\n\u001b[32m     63\u001b[39m tokenizer.deprecation_warnings[\u001b[33m\"\u001b[39m\u001b[33mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m     padded = \u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpad_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpad_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     68\u001b[39m     \u001b[38;5;66;03m# Restore the state of the warning.\u001b[39;00m\n\u001b[32m     69\u001b[39m     tokenizer.deprecation_warnings[\u001b[33m\"\u001b[39m\u001b[33mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[33m\"\u001b[39m] = warning_state\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:3509\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.pad\u001b[39m\u001b[34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, padding_side, return_attention_mask, return_tensors, verbose)\u001b[39m\n\u001b[32m   3507\u001b[39m \u001b[38;5;66;03m# The model's main input name, usually `input_ids`, has been passed for padding\u001b[39;00m\n\u001b[32m   3508\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model_input_names[\u001b[32m0\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m encoded_inputs:\n\u001b[32m-> \u001b[39m\u001b[32m3509\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   3510\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou should supply an encoding or a list of encodings to this method \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3511\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mthat includes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.model_input_names[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, but you provided \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(encoded_inputs.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   3512\u001b[39m     )\n\u001b[32m   3514\u001b[39m required_input = encoded_inputs[\u001b[38;5;28mself\u001b[39m.model_input_names[\u001b[32m0\u001b[39m]]\n\u001b[32m   3516\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m required_input \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(required_input, Sized) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(required_input) == \u001b[32m0\u001b[39m):\n",
      "\u001b[31mValueError\u001b[39m: You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided ['text']"
     ]
    }
   ],
   "source": [
    "# # ============================================================================\n",
    "# # CELL 11: TRAIN THE MODEL\n",
    "# # ============================================================================\n",
    "\n",
    "# print(\"\\n\" + \"=\" * 70)\n",
    "# print(\"üöÄ STARTING Latin American CULTURAL MODEL TRAINING\")\n",
    "# print(\"=\" * 70)\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args_latin,\n",
    "#     train_dataset=combined_dataset,\n",
    "#     data_collator=data_collator,\n",
    "# )\n",
    "\n",
    "# # Train\n",
    "# trainer.train()\n",
    "\n",
    "# print(\"\\n‚úÖ Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8991f10e-2ea3-4fde-914e-80a9fc6a8c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Adapter saved at: ./26Dec2025/african_cultural_model/adapter\n",
      "üìÇ Files:\n",
      " - README.md\n",
      " - adapter_model.safetensors\n",
      " - adapter_config.json\n",
      " - chat_template.jinja\n",
      " - tokenizer_config.json\n",
      " - special_tokens_map.json\n",
      " - tokenizer.json\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 14: SAVE TRAINED LoRA ADAPTER\n",
    "# ============================================================================\n",
    "\n",
    "adapter_path = os.path.join(config.output_dir, \"adapter\")\n",
    "os.makedirs(adapter_path, exist_ok=True)\n",
    "\n",
    "model.save_pretrained(adapter_path)\n",
    "tokenizer.save_pretrained(adapter_path)\n",
    "\n",
    "print(f\"‚úÖ Adapter saved at: {adapter_path}\")\n",
    "print(\"üìÇ Files:\")\n",
    "for f in os.listdir(adapter_path):\n",
    "    print(\" -\", f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088c8efc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f783bfc2",
   "metadata": {},
   "source": [
    "#26Dec2025 started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c336eb03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a63bd0dd57564402828ca6fa5c8eb01d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# AFRICAN_MODEL_PATH = \"/content/drive/MyDrive/nDNA_amitava_das/FinetunedModels/26Dec2025/african_cultural_model/adapter/\"  # change as needed\n",
    "# LATAM_MODEL_PATH = \"/content/drive/MyDrive/nDNA_amitava_das/FinetunedModels/26Dec2025/latam_cultural_model/adapter/\"  # change as needed\n",
    "\n",
    "# AFRICAN_MODEL_PATH = \"./26Dec2025/african_cultural_model/adapter/\"  # change as needed\n",
    "\n",
    "base_model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "LATAM_MODEL_PATH = \"./26Dec2025/latam_cultural_model/adapter/\"  # change as needed\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id, use_fast=False)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# african_model = AutoModelForCausalLM.from_pretrained(\n",
    "#     AFRICAN_MODEL_PATH,\n",
    "#     torch_dtype=torch.float32,\n",
    "#     device_map=\"auto\",\n",
    "#     output_hidden_states=True\n",
    "# ).eval()\n",
    "\n",
    "latam_model = AutoModelForCausalLM.from_pretrained(\n",
    "    LATAM_MODEL_PATH,\n",
    "    torch_dtype=torch.float32,\n",
    "    device_map=\"auto\",\n",
    "    output_hidden_states=True\n",
    ").eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e32ca9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LATAM_PROBES = [\n",
    "    \"Describe the cultural significance of the Day of the Dead in Latin America.\",\n",
    "    \"Explain indigenous traditions in Latin American societies.\",\n",
    "    \"Describe the role of family in Latin American culture.\",\n",
    "    \"Explain community and social relationships in Latin America.\",\n",
    "    \"Describe traditional celebrations in Latin American countries.\",\n",
    "    \"Explain the influence of indigenous cultures on Latin America.\",\n",
    "    \"Describe cultural identity in Latin American societies.\",\n",
    "    \"Explain the role of religion in Latin American daily life.\",\n",
    "    \"Describe Latin American approaches to community solidarity.\",\n",
    "    \"Explain the importance of festivals in Latin American culture.\",\n",
    "\n",
    "    \"Describe musical traditions across Latin America.\",\n",
    "    \"Explain the cultural role of dance in Latin American societies.\",\n",
    "    \"Describe traditional Latin American artistic expressions.\",\n",
    "    \"Explain how history shapes Latin American cultural identity.\",\n",
    "    \"Describe oral and written storytelling traditions in Latin America.\",\n",
    "    \"Explain the influence of colonial history on Latin American culture.\",\n",
    "    \"Describe traditional family roles in Latin America.\",\n",
    "    \"Explain Latin American views on community responsibility.\",\n",
    "    \"Describe the cultural importance of food in Latin America.\",\n",
    "    \"Explain how cultural values are passed between generations.\",\n",
    "\n",
    "    \"Describe indigenous languages and their cultural significance in Latin America.\",\n",
    "    \"Explain the concept of mestizaje in Latin American societies.\",\n",
    "    \"Describe Afro-Latin cultural influences in Latin America.\",\n",
    "    \"Explain cultural diversity within Latin American countries.\",\n",
    "    \"Describe the role of art in expressing Latin American identity.\",\n",
    "    \"Explain the cultural significance of murals in Latin America.\",\n",
    "    \"Describe Latin American literary traditions.\",\n",
    "    \"Explain the importance of magical realism in Latin American literature.\",\n",
    "    \"Describe storytelling themes common in Latin American culture.\",\n",
    "    \"Explain how cultural memory is preserved in Latin America.\",\n",
    "\n",
    "    \"Describe Latin American perspectives on nature and land.\",\n",
    "    \"Explain the relationship between culture and geography in Latin America.\",\n",
    "    \"Describe rural and urban cultural differences in Latin America.\",\n",
    "    \"Explain traditional healing and folk medicine in Latin America.\",\n",
    "    \"Describe cultural rituals associated with life events in Latin America.\",\n",
    "    \"Explain how Latin American cultures approach death and remembrance.\",\n",
    "    \"Describe the role of music in Latin American social life.\",\n",
    "    \"Explain how dance expresses cultural identity in Latin America.\",\n",
    "    \"Describe the importance of community gatherings in Latin America.\",\n",
    "    \"Explain cultural symbolism in Latin American art.\",\n",
    "\n",
    "    \"Describe how Latin American traditions adapt to modern society.\",\n",
    "    \"Explain cultural continuity across generations in Latin America.\",\n",
    "    \"Describe Latin American approaches to resilience and social change.\",\n",
    "    \"Explain how collective identity is formed in Latin America.\",\n",
    "    \"Describe the influence of migration on Latin American culture.\",\n",
    "    \"Explain cultural expressions of joy and celebration in Latin America.\",\n",
    "    \"Describe the role of storytelling in shaping Latin American values.\",\n",
    "    \"Explain how traditions maintain social cohesion in Latin America.\",\n",
    "    \"Describe Latin American perspectives on cultural heritage.\",\n",
    "    \"Explain how culture shapes everyday behavior in Latin America.\",\n",
    "\n",
    "    \"Describe the relationship between tradition and modernity in Latin America.\",\n",
    "    \"Explain how cultural practices reflect shared values in Latin America.\",\n",
    "    \"Describe how identity is expressed in Latin American communities.\",\n",
    "    \"Explain the role of memory and history in Latin American culture.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b8cdb072",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.autograd.grad_mode.set_grad_enabled(mode=False)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_LAYERS = 32  # explicit, per your requirement\n",
    "TOKENS_PER_EX = 16  # Method-5 default\n",
    "layer_idx = 16\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3a36f87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fr_embed(q, eps=1e-9):\n",
    "    q = torch.clamp(q, min=eps)\n",
    "    u = torch.sqrt(q)\n",
    "    return u / torch.norm(u)\n",
    "\n",
    "def fr_distance(u1, u2):\n",
    "    cos = torch.clamp(torch.dot(u1, u2), -1 + 1e-7, 1 - 1e-7)\n",
    "    return 2.0 * torch.arccos(cos)\n",
    "\n",
    "def tangent_norm(u_prev, u_next):\n",
    "    proj = u_next - torch.dot(u_next, u_prev) * u_prev\n",
    "    return torch.norm(proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ec567feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layerwise_thermo(model, tokenizer, prompt, layer_idx):\n",
    "    inp = tokenizer(prompt, return_tensors=\"pt\",\n",
    "                    truncation=True, max_length=256).to(device)\n",
    "    out = model(**inp)\n",
    "\n",
    "    lm_head = model.lm_head\n",
    "    h = out.hidden_states[layer_idx].squeeze(0)  # [T, D]\n",
    "\n",
    "    T = min(TOKENS_PER_EX, h.shape[0] - 1)\n",
    "    u_list = []\n",
    "\n",
    "    for t in range(T):\n",
    "        logits_t = lm_head(h[t])\n",
    "        probs_t = F.softmax(logits_t, dim=-1)\n",
    "        u_list.append(fr_embed(probs_t))\n",
    "\n",
    "    delta = 0.0\n",
    "    for i in range(len(u_list) - 1):\n",
    "        delta += fr_distance(u_list[i], u_list[i+1])\n",
    "\n",
    "    return delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c4dfecf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layerwise_belief(model, tokenizer, prompt, layer_idx):\n",
    "    inp = tokenizer(prompt, return_tensors=\"pt\",\n",
    "                    truncation=True, max_length=256).to(device)\n",
    "    out = model(**inp)\n",
    "\n",
    "    lm_head = model.lm_head\n",
    "    h = out.hidden_states[layer_idx].squeeze(0)\n",
    "\n",
    "    T = min(TOKENS_PER_EX, h.shape[0] - 1)\n",
    "    u_list = []\n",
    "\n",
    "    for t in range(T):\n",
    "        logits_t = lm_head(h[t])\n",
    "        probs_t = F.softmax(logits_t, dim=-1)\n",
    "        u_list.append(fr_embed(probs_t))\n",
    "\n",
    "    belief = 0.0\n",
    "    for i in range(len(u_list) - 1):\n",
    "        belief += tangent_norm(u_list[i], u_list[i+1])\n",
    "\n",
    "    return belief\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d5c4ae1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectral_curvature(hidden, eps=1e-6):\n",
    "    X = hidden - hidden.mean(dim=0, keepdim=True)\n",
    "    cov = (X.T @ X) / (X.shape[0] - 1)\n",
    "    cov = cov + eps * torch.eye(cov.shape[0], device=cov.device)\n",
    "    eigvals = torch.linalg.eigvalsh(cov)\n",
    "    eigvals = torch.clamp(eigvals, min=1e-8)\n",
    "    return torch.log(eigvals).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2dc800fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef0a88bb3e84466caebfe1dd95df1d32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Layer sweep:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[54]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m thermo_vals, belief_vals, spectral_vals = [], [], []\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m LATAM_PROBES:\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     thermo_vals.append(\u001b[43mlayerwise_thermo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     10\u001b[39m     belief_vals.append(layerwise_belief(model, tokenizer, prompt, l))\n\u001b[32m     12\u001b[39m     inp = tokenizer(prompt, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m).to(device)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mlayerwise_thermo\u001b[39m\u001b[34m(model, tokenizer, prompt, layer_idx)\u001b[39m\n\u001b[32m      4\u001b[39m out = model(**inp)\n\u001b[32m      6\u001b[39m lm_head = model.lm_head\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m h = \u001b[43mout\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[43m]\u001b[49m.squeeze(\u001b[32m0\u001b[39m)  \u001b[38;5;66;03m# [T, D]\u001b[39;00m\n\u001b[32m      9\u001b[39m T = \u001b[38;5;28mmin\u001b[39m(TOKENS_PER_EX, h.shape[\u001b[32m0\u001b[39m] - \u001b[32m1\u001b[39m)\n\u001b[32m     10\u001b[39m u_list = []\n",
      "\u001b[31mTypeError\u001b[39m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "thermo = torch.zeros(NUM_LAYERS, device=device)\n",
    "belief = torch.zeros(NUM_LAYERS, device=device)\n",
    "spectral = torch.zeros(NUM_LAYERS, device=device)\n",
    "\n",
    "for l in tqdm(range(NUM_LAYERS), desc=\"Layer sweep\"):\n",
    "    thermo_vals, belief_vals, spectral_vals = [], [], []\n",
    "\n",
    "    for prompt in LATAM_PROBES:\n",
    "        thermo_vals.append(layerwise_thermo(model, tokenizer, prompt, l))\n",
    "        belief_vals.append(layerwise_belief(model, tokenizer, prompt, l))\n",
    "\n",
    "        inp = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "        out = model(**inp)\n",
    "        h_l = out.hidden_states[l].squeeze(0)\n",
    "        spectral_vals.append(spectral_curvature(h_l))\n",
    "\n",
    "    thermo[l] = torch.stack(thermo_vals).mean()\n",
    "    belief[l] = torch.stack(belief_vals).mean()\n",
    "    spectral[l] = torch.stack(spectral_vals).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8c10c3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm01(x):\n",
    "    return (x - x.min()) / (x.max() - x.min() + 1e-8)\n",
    "\n",
    "thermo_n = norm01(thermo)\n",
    "belief_n = norm01(belief)\n",
    "spectral_n = norm01(spectral)\n",
    "\n",
    "ndna_layer = thermo_n * belief_n * spectral_n\n",
    "ndna_cum = torch.cumsum(ndna_layer, dim=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e1d191",
   "metadata": {},
   "source": [
    "Thermo vs Belief vs Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fb0f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = torch.arange(NUM_LAYERS)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter3d(\n",
    "    x=belief_n.cpu(),\n",
    "    y=thermo_n.cpu(),\n",
    "    z=layers.cpu(),\n",
    "    mode=\"lines+markers\",\n",
    "    line=dict(width=5),\n",
    "    marker=dict(size=5)\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Thermodynamic Length vs Belief vs Layer (Method-5)\",\n",
    "    scene=dict(\n",
    "        xaxis_title=\"Belief ‚Äñt‚Ñì‚Äñ\",\n",
    "        yaxis_title=\"Thermodynamic Length Œî‚Ñì\",\n",
    "        zaxis_title=\"Layer\"\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd00f44",
   "metadata": {},
   "source": [
    "Joint evolution of curvature and thermodynamic length across depth, revealing early-layer curvature dominance and late-layer geometric flattening."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cad4235",
   "metadata": {},
   "source": [
    "Spectral vs Thermo vs Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45d409b",
   "metadata": {},
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter3d(\n",
    "    x=spectral_n.cpu(),\n",
    "    y=thermo_n.cpu(),\n",
    "    z=layers.cpu(),\n",
    "    mode=\"lines+markers\"\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Spectral Curvature vs Thermodynamic Length vs Layer\",\n",
    "    scene=dict(\n",
    "        xaxis_title=\"Spectral Curvature Œ∫‚Ñì\",\n",
    "        yaxis_title=\"Thermodynamic Length Œî‚Ñì\",\n",
    "        zaxis_title=\"Layer\"\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4723f18a",
   "metadata": {},
   "source": [
    "Joint evolution of curvature and thermodynamic length across depth, revealing early-layer curvature dominance and late-layer geometric flattening."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43096f12",
   "metadata": {},
   "source": [
    "nDNA per Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bf4fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = go.Figure()\n",
    "# fig.add_trace(go.Scatter(\n",
    "#     x=layers.cpu(),\n",
    "#     y=ndna_layer.cpu(),\n",
    "#     mode=\"lines+markers\"\n",
    "# ))\n",
    "\n",
    "# fig.update_layout(\n",
    "#     title=\"Layerwise nDNA Score\",\n",
    "#     xaxis_title=\"Layer\",\n",
    "#     yaxis_title=\"nDNA Score\"\n",
    "# )\n",
    "\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a7306d",
   "metadata": {},
   "source": [
    "Layerwise neural DNA (nDNA) score, combining thermodynamic, belief, and spectral components, highlighting the layers contributing most to cultural specialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3757621",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5523cc09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcca249f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cd862998",
   "metadata": {},
   "source": [
    "26dEC2025 end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "89403d19-a136-441e-a74a-6b7d0a968adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Belief: token-wise entropy (Method-5)\n",
    "# ------------------------------------------------\n",
    "def belief_entropy(logits: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    logits: [T, V]\n",
    "    returns: Œ≤(t) ‚àà R^T\n",
    "    \"\"\"\n",
    "    p = F.softmax(logits, dim=-1)\n",
    "    return -(p * torch.log(p + 1e-9)).sum(dim=-1)\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Thermodynamic length: Fisher‚ÄìRao (Method-5)\n",
    "# ------------------------------------------------\n",
    "def fisher_rao_thermo_length(logits: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    logits: [T, V]\n",
    "    returns: cumulative ‚Ñí(t) ‚àà R^T (monotonic)\n",
    "    \"\"\"\n",
    "    logp = F.log_softmax(logits, dim=-1)\n",
    "    p = F.softmax(logits, dim=-1)\n",
    "\n",
    "    delta = logp[1:] - logp[:-1]          # [T-1, V]\n",
    "    fisher = p[:-1]                       # [T-1, V]\n",
    "\n",
    "    ds = torch.sqrt((fisher * delta**2).sum(dim=-1))\n",
    "    thermo = torch.cat([\n",
    "        torch.zeros(1, device=logits.device),\n",
    "        torch.cumsum(ds, dim=0)\n",
    "    ])\n",
    "    return thermo\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Spectral curvature: stable Method-5\n",
    "# ------------------------------------------------\n",
    "def spectral_signature(hidden_states: torch.Tensor, eps=1e-5) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    hidden_states: [T, D] for ONE layer\n",
    "    returns: scalar Œ∫\n",
    "    \"\"\"\n",
    "    T, D = hidden_states.shape\n",
    "    if T < 4:\n",
    "        return torch.tensor(0.0, device=hidden_states.device)\n",
    "\n",
    "    X = hidden_states - hidden_states.mean(dim=0, keepdim=True)\n",
    "    cov = (X.T @ X) / (T - 1)\n",
    "    cov = cov + eps * torch.eye(D, device=cov.device)\n",
    "\n",
    "    eigvals = torch.linalg.eigvalsh(cov)\n",
    "    eigvals = torch.clamp(eigvals, min=1e-8)\n",
    "\n",
    "    return torch.log(eigvals).mean()\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# FULL Method-5 trajectory (SAFE VERSION)\n",
    "# ------------------------------------------------\n",
    "def ndna_method5_trajectory(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompts,\n",
    "    layer_idx: int = 16,\n",
    "    min_tokens: int = 1,   # üîë CRITICAL FIX\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns concatenated (belief, spectral, thermo)\n",
    "    Guaranteed non-empty or raises clear error\n",
    "    \"\"\"\n",
    "\n",
    "    all_b, all_s, all_t = [], [], []\n",
    "\n",
    "    for prompt in prompts:\n",
    "        inputs = tokenizer(\n",
    "            prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=256\n",
    "        ).to(model.device)\n",
    "\n",
    "        T = inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "        # üîë DO NOT SKIP SHORT PROMPTS\n",
    "        if T < min_tokens:\n",
    "            continue\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = model(**inputs)\n",
    "            print(out)\n",
    "\n",
    "        logits = out.logits.squeeze(0)                    # [T, V]\n",
    "        print(logits)\n",
    "        hidden = out.hidden_states[layer_idx].squeeze(0) # [T, D]\n",
    "        print(hidden)\n",
    "\n",
    "        belief = belief_entropy(logits)\n",
    "        thermo = fisher_rao_thermo_length(logits)\n",
    "        spectral = spectral_signature(hidden)\n",
    "\n",
    "        all_b.append(belief)\n",
    "        print(all_b)\n",
    "        all_t.append(thermo)\n",
    "        print(all_t)\n",
    "        all_s.append(spectral.repeat(len(belief)))\n",
    "        print(all_s)\n",
    "\n",
    "    # üîí HARD SAFETY CHECK (PROFESSOR-APPROVED)\n",
    "    if len(all_b) == 0 or len(all_t) == 0 or len(all_s) == 0 :\n",
    "        raise RuntimeError(\n",
    "            \"Method-5 failure: no valid prompts produced geometry. \"\n",
    "            \"Check tokenization and min_tokens.\"\n",
    "        )\n",
    "\n",
    "    return (\n",
    "        torch.cat(all_b),\n",
    "        torch.cat(all_s),\n",
    "        torch.cat(all_t),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a0fd81fe-22a3-4d3f-88c0-2769755e90d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def belief_entropy(logits):\n",
    "#     p = torch.softmax(logits, dim=-1)\n",
    "#     return -(p * torch.log(p + 1e-9)).sum(-1)\n",
    "\n",
    "# def thermo_length(logits):\n",
    "#     logp = torch.log_softmax(logits, dim=-1)\n",
    "#     delta = logp[1:] - logp[:-1]\n",
    "#     fisher = torch.softmax(logits[:-1], -1)\n",
    "#     ds = torch.sqrt((fisher * delta**2).sum(-1))\n",
    "#     return torch.cat([torch.zeros(1, device=ds.device), ds.cumsum(0)])\n",
    "\n",
    "# def spectral_signature(hidden, eps=1e-5):\n",
    "#     X = hidden - hidden.mean(0, keepdim=True)\n",
    "#     if X.shape[0] < 10:\n",
    "#         return torch.tensor(0.0, device=X.device)\n",
    "#     cov = (X.T @ X)/(X.shape[0]-1) + eps*torch.eye(X.shape[1], device=X.device)\n",
    "#     eig = torch.linalg.eigvalsh(cov)\n",
    "#     return torch.log(torch.clamp(eig,1e-8)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "eb5ddf92-f199-497a-83eb-6f3387d5d106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ndna_trajectory(model, prompts, layer_idx=16):\n",
    "#     all_b, all_t, all_s = [], [], []\n",
    "\n",
    "#     for p in prompts:\n",
    "#         inp = tokenizer(p, return_tensors=\"pt\").to(DEVICE)\n",
    "#         out = model(**inp)\n",
    "\n",
    "#         logits = out.logits.squeeze(0)\n",
    "#         belief = belief_entropy(logits)\n",
    "#         thermo = thermo_length(logits)\n",
    "\n",
    "#         hidden = out.hidden_states[layer_idx].squeeze(0)\n",
    "#         spec = spectral_signature(hidden).repeat(len(belief))\n",
    "\n",
    "#         all_b.append(belief)\n",
    "#         all_t.append(thermo)\n",
    "#         all_s.append(spec)\n",
    "\n",
    "#     return (\n",
    "#         torch.cat(all_b),\n",
    "#         torch.cat(all_s),\n",
    "#         torch.cat(all_t)\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "58b1cf72-475c-40ce-8b2a-bb2e72f52187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ndna_method5_trajectory(\n",
    "#     model,\n",
    "#     tokenizer,\n",
    "#     prompts,\n",
    "#     layer_idx: int = 16,\n",
    "#     min_tokens: int = 16,\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Returns concatenated (belief, spectral, thermo) trajectory\n",
    "#     Geometry lives in (Œ≤, Œ∫, L)\n",
    "#     \"\"\"\n",
    "#     all_b, all_s, all_t = [], [], []\n",
    "\n",
    "#     for prompt in prompts:\n",
    "#         inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "#         if inputs[\"input_ids\"].shape[1] < min_tokens:\n",
    "#             continue\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             out = model(**inputs)\n",
    "\n",
    "#         logits = out.logits.squeeze(0)                      # [T, V]\n",
    "#         hidden = out.hidden_states[layer_idx].squeeze(0)   # [T, D]\n",
    "\n",
    "#         belief = belief_entropy(logits)                     # [T]\n",
    "#         thermo = fisher_rao_thermo_length(logits)           # [T]\n",
    "#         spectral = spectral_signature(hidden)               # scalar\n",
    "\n",
    "#         all_b.append(belief)\n",
    "#         all_t.append(thermo)\n",
    "#         all_s.append(spectral.repeat(len(belief)))\n",
    "\n",
    "#     return (\n",
    "#         torch.cat(all_b),\n",
    "#         torch.cat(all_s),\n",
    "#         torch.cat(all_t),\n",
    "#     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e2bcabd1-7662-43a2-bde2-7aed72f51b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "AFRICA_ADAPTER_PATH = \"/latest_african_cultural_model/adapter\"\n",
    "LATAM_ADAPTER_PATH  = \"/latest_latin_cultural_model/adapter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c0439460-bded-4a80-9e0c-ea6750c2d245",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82821eba26ec41e790711de4f4cb1c5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "BASE_MODEL_ID = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# FP32 base (geometry-safe)\n",
    "base_fp32 = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_ID,\n",
    "    torch_dtype=torch.float32,\n",
    "    output_hidden_states=True,\n",
    "    device_map=\"auto\"\n",
    ").eval()\n",
    "\n",
    "africa_model = PeftModel.from_pretrained(\n",
    "    base_fp32,\n",
    "    AFRICA_ADAPTER_PATH,\n",
    "    is_trainable=False\n",
    ").eval()\n",
    "\n",
    "latam_model = PeftModel.from_pretrained(\n",
    "    base_fp32,\n",
    "    LATAM_ADAPTER_PATH,\n",
    "    is_trainable=False\n",
    ").eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "25e0bc42-1031-43bb-93b0-ec2055b68ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fisher_style_merge_lora(\n",
    "    base_model,\n",
    "    adapter_path_a,\n",
    "    adapter_path_b,\n",
    "    alpha: float = 0.5,\n",
    "):\n",
    "    \"\"\"\n",
    "    Geometry-aware (delta-space) merge of two LoRA adapters.\n",
    "    alpha ‚àà [0,1] controls cultural dominance.\n",
    "    \"\"\"\n",
    "\n",
    "    mA = PeftModel.from_pretrained(\n",
    "        base_model, adapter_path_a, is_trainable=False\n",
    "    )\n",
    "    mB = PeftModel.from_pretrained(\n",
    "        base_model, adapter_path_b, is_trainable=False\n",
    "    )\n",
    "\n",
    "    sdA = mA.state_dict()\n",
    "    sdB = mB.state_dict()\n",
    "\n",
    "    merged_sd = {}\n",
    "    for k in sdA:\n",
    "        if k in sdB:\n",
    "            merged_sd[k] = alpha * sdA[k] + (1 - alpha) * sdB[k]\n",
    "        else:\n",
    "            merged_sd[k] = sdA[k]\n",
    "\n",
    "    offspring = PeftModel.from_pretrained(\n",
    "        base_model, adapter_path_a, is_trainable=False\n",
    "    )\n",
    "    offspring.load_state_dict(merged_sd, strict=False)\n",
    "    return offspring.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "019944c0-6416-4f4e-a6c7-58e83ec47ba3",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.37 GiB of which 1.69 MiB is free. Process 328458 has 47.36 GiB memory in use. Of the allocated memory 42.15 GiB is allocated by PyTorch, and 4.81 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[55]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m offspring_model = \u001b[43mfisher_style_merge_lora\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbase_fp32\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mAFRICA_ADAPTER_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mLATAM_ADAPTER_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.5\u001b[39;49m\n\u001b[32m      6\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[54]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mfisher_style_merge_lora\u001b[39m\u001b[34m(base_model, adapter_path_a, adapter_path_b, alpha)\u001b[39m\n\u001b[32m     26\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     27\u001b[39m         merged_sd[k] = sdA[k]\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m offspring = \u001b[43mPeftModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_path_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_trainable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m     31\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m offspring.load_state_dict(merged_sd, strict=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m offspring.eval()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/peft/peft_model.py:568\u001b[39m, in \u001b[36mPeftModel.from_pretrained\u001b[39m\u001b[34m(cls, model, model_id, adapter_name, is_trainable, config, autocast_adapter_dtype, ephemeral_gpu_offload, low_cpu_mem_usage, key_mapping, **kwargs)\u001b[39m\n\u001b[32m    559\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    560\u001b[39m     model = MODEL_TYPE_TO_PEFT_MODEL_MAPPING[config.task_type](\n\u001b[32m    561\u001b[39m         model,\n\u001b[32m    562\u001b[39m         config,\n\u001b[32m   (...)\u001b[39m\u001b[32m    565\u001b[39m         low_cpu_mem_usage=low_cpu_mem_usage,\n\u001b[32m    566\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m568\u001b[39m load_result = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_adapter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    569\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    570\u001b[39m \u001b[43m    \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    571\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_trainable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_trainable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    572\u001b[39m \u001b[43m    \u001b[49m\u001b[43mautocast_adapter_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mautocast_adapter_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    573\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    574\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    575\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    576\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    578\u001b[39m \u001b[38;5;66;03m# 1. Remove VB-LoRA vector bank, since it's a shared parameter set via the VBLoRAModel\u001b[39;00m\n\u001b[32m    579\u001b[39m \u001b[38;5;66;03m# 2. Remove the prompt encoder, as it does not need to be part of the checkpoint\u001b[39;00m\n\u001b[32m    580\u001b[39m missing_keys = [\n\u001b[32m    581\u001b[39m     k \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m load_result.missing_keys \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mvblora_vector_bank\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m k \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mprompt_encoder\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m k\n\u001b[32m    582\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/peft/peft_model.py:1362\u001b[39m, in \u001b[36mPeftModel.load_adapter\u001b[39m\u001b[34m(self, model_id, adapter_name, is_trainable, torch_device, autocast_adapter_dtype, ephemeral_gpu_offload, low_cpu_mem_usage, key_mapping, **kwargs)\u001b[39m\n\u001b[32m   1359\u001b[39m     peft_config.inference_mode = \u001b[38;5;129;01mnot\u001b[39;00m is_trainable\n\u001b[32m   1360\u001b[39m     \u001b[38;5;28mself\u001b[39m.add_adapter(adapter_name, peft_config, low_cpu_mem_usage=low_cpu_mem_usage)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m adapters_weights = \u001b[43mload_peft_weights\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhf_hub_download_kwargs\u001b[49m\n\u001b[32m   1364\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1366\u001b[39m \u001b[38;5;66;03m# load the weights into the model\u001b[39;00m\n\u001b[32m   1367\u001b[39m ignore_mismatched_sizes = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mignore_mismatched_sizes\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/peft/utils/save_and_load.py:693\u001b[39m, in \u001b[36mload_peft_weights\u001b[39m\u001b[34m(model_id, device, key_mapping, **hf_hub_download_kwargs)\u001b[39m\n\u001b[32m    691\u001b[39m         adapters_weights = safe_load_file(filename, device=\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    692\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m693\u001b[39m         adapters_weights = \u001b[43msafe_load_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    694\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    695\u001b[39m     adapters_weights = torch_load(filename, map_location=torch.device(device))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/safetensors/torch.py:338\u001b[39m, in \u001b[36mload_file\u001b[39m\u001b[34m(filename, device)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m safe_open(filename, framework=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m, device=device) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    337\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m f.offset_keys():\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m         result[k] = \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    339\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 47.37 GiB of which 1.69 MiB is free. Process 328458 has 47.36 GiB memory in use. Of the allocated memory 42.15 GiB is allocated by PyTorch, and 4.81 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "offspring_model = fisher_style_merge_lora(\n",
    "    base_fp32,\n",
    "    AFRICA_ADAPTER_PATH,\n",
    "    LATAM_ADAPTER_PATH,\n",
    "    alpha=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec0b88f-6c32-41ef-9b79-8c98d2441fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for nDNA analysis 64 prompts\n",
    "'''\n",
    "These probes are never used for training. Only used for geometry.\n",
    "'''\n",
    "\n",
    "AFRICA_PROBES = [\n",
    "    \"Explain clan and kinship systems in traditional African communities.\",\n",
    "    \"Describe the importance of elders in African social structures.\",\n",
    "    \"Explain African concepts of community and collective identity.\",\n",
    "    \"Describe traditional African belief systems and spirituality.\",\n",
    "    \"Explain the role of ancestors in African cultural traditions.\",\n",
    "    \"Describe initiation and coming-of-age rituals in Africa.\",\n",
    "    \"Explain the role of music and rhythm in African daily life.\",\n",
    "    \"Describe the cultural significance of drums in Africa.\",\n",
    "\n",
    "    \"Describe marriage and family structures in African societies.\",\n",
    "    \"Explain the role of proverbs in African oral traditions.\",\n",
    "    \"Describe traditional leadership and chieftaincy systems.\",\n",
    "    \"Explain the importance of land and ancestry in African culture.\",\n",
    "    \"Describe African concepts of time and continuity.\",\n",
    "    \"Explain how history is preserved in African oral traditions.\",\n",
    "    \"Describe African approaches to education and learning.\",\n",
    "    \"Explain the role of storytelling in African moral education.\",\n",
    "    \"Describe traditional African festivals and ceremonies.\",\n",
    "\n",
    "    \"Explain the cultural meaning of masks in African societies.\",\n",
    "    \"Describe African artistic traditions and symbolism.\",\n",
    "    \"Explain the role of dance in African cultural expression.\",\n",
    "    \"Describe the social function of African music.\",\n",
    "    \"Explain the cultural importance of communal labor in Africa.\",\n",
    "    \"Describe African hospitality and social etiquette.\",\n",
    "    \"Explain the role of spirituality in everyday African life.\",\n",
    "    \"Describe traditional African healing practices.\",\n",
    "    \"Explain how myths function in African cultures.\",\n",
    "    \"Describe the role of griots in West African societies.\",\n",
    "\n",
    "    \"Explain the significance of lineage in African identity.\",\n",
    "    \"Describe African perspectives on individuality and community.\",\n",
    "    \"Explain how cultural values are transmitted across generations.\",\n",
    "    \"Describe African views on nature and the environment.\",\n",
    "    \"Explain the role of rituals in maintaining social harmony.\",\n",
    "    \"Describe traditional African approaches to justice.\",\n",
    "    \"Explain the cultural meaning of names in African societies.\",\n",
    "    \"Describe the symbolism of animals in African folklore.\",\n",
    "    \"Explain African perspectives on life cycles and death.\",\n",
    "    \"Describe traditional African wedding customs.\",\n",
    "\n",
    "    \"Explain how African societies understand social responsibility.\",\n",
    "    \"Describe the role of respect and hierarchy in African culture.\",\n",
    "    \"Explain African communal decision-making processes.\",\n",
    "    \"Describe traditional African food-sharing practices.\",\n",
    "    \"Explain how African cultures define personal identity.\",\n",
    "    \"Describe the importance of community memory in Africa.\",\n",
    "    \"Explain how African cultures view knowledge and wisdom.\",\n",
    "    \"Describe traditional African rites of passage.\",\n",
    "    \"Explain African perspectives on harmony and balance.\",\n",
    "    \"Describe the cultural role of storytelling during gatherings.\",\n",
    "\n",
    "    \"Explain how African traditions adapt to modern life.\",\n",
    "    \"Describe continuity between ancient and modern African cultures.\",\n",
    "    \"Explain African approaches to resilience and survival.\",\n",
    "    \"Describe how cultural values guide African social behavior.\"\n",
    "]\n",
    "\n",
    "LATAM_PROBES = [\n",
    "    \"Describe the cultural significance of the Day of the Dead in Latin America.\",\n",
    "    \"Explain indigenous traditions in Latin American societies.\",\n",
    "    \"Describe the role of family in Latin American culture.\",\n",
    "    \"Explain community and social relationships in Latin America.\",\n",
    "    \"Describe traditional celebrations in Latin American countries.\",\n",
    "    \"Explain the influence of indigenous cultures on Latin America.\",\n",
    "    \"Describe cultural identity in Latin American societies.\",\n",
    "    \"Explain the role of religion in Latin American daily life.\",\n",
    "    \"Describe Latin American approaches to community solidarity.\",\n",
    "    \"Explain the importance of festivals in Latin American culture.\",\n",
    "\n",
    "    \"Describe musical traditions across Latin America.\",\n",
    "    \"Explain the cultural role of dance in Latin American societies.\",\n",
    "    \"Describe traditional Latin American artistic expressions.\",\n",
    "    \"Explain how history shapes Latin American cultural identity.\",\n",
    "    \"Describe oral and written storytelling traditions in Latin America.\",\n",
    "    \"Explain the influence of colonial history on Latin American culture.\",\n",
    "    \"Describe traditional family roles in Latin America.\",\n",
    "    \"Explain Latin American views on community responsibility.\",\n",
    "    \"Describe the cultural importance of food in Latin America.\",\n",
    "    \"Explain how cultural values are passed between generations.\",\n",
    "\n",
    "    \"Describe indigenous languages and their cultural significance in Latin America.\",\n",
    "    \"Explain the concept of mestizaje in Latin American societies.\",\n",
    "    \"Describe Afro-Latin cultural influences in Latin America.\",\n",
    "    \"Explain cultural diversity within Latin American countries.\",\n",
    "    \"Describe the role of art in expressing Latin American identity.\",\n",
    "    \"Explain the cultural significance of murals in Latin America.\",\n",
    "    \"Describe Latin American literary traditions.\",\n",
    "    \"Explain the importance of magical realism in Latin American literature.\",\n",
    "    \"Describe storytelling themes common in Latin American culture.\",\n",
    "    \"Explain how cultural memory is preserved in Latin America.\",\n",
    "\n",
    "    \"Describe Latin American perspectives on nature and land.\",\n",
    "    \"Explain the relationship between culture and geography in Latin America.\",\n",
    "    \"Describe rural and urban cultural differences in Latin America.\",\n",
    "    \"Explain traditional healing and folk medicine in Latin America.\",\n",
    "    \"Describe cultural rituals associated with life events in Latin America.\",\n",
    "    \"Explain how Latin American cultures approach death and remembrance.\",\n",
    "    \"Describe the role of music in Latin American social life.\",\n",
    "    \"Explain how dance expresses cultural identity in Latin America.\",\n",
    "    \"Describe the importance of community gatherings in Latin America.\",\n",
    "    \"Explain cultural symbolism in Latin American art.\",\n",
    "\n",
    "    \"Describe how Latin American traditions adapt to modern society.\",\n",
    "    \"Explain cultural continuity across generations in Latin America.\",\n",
    "    \"Describe Latin American approaches to resilience and social change.\",\n",
    "    \"Explain how collective identity is formed in Latin America.\",\n",
    "    \"Describe the influence of migration on Latin American culture.\",\n",
    "    \"Explain cultural expressions of joy and celebration in Latin America.\",\n",
    "    \"Describe the role of storytelling in shaping Latin American values.\",\n",
    "    \"Explain how traditions maintain social cohesion in Latin America.\",\n",
    "    \"Describe Latin American perspectives on cultural heritage.\",\n",
    "    \"Explain how culture shapes everyday behavior in Latin America.\",\n",
    "\n",
    "    \"Describe the relationship between tradition and modernity in Latin America.\",\n",
    "    \"Explain how cultural practices reflect shared values in Latin America.\",\n",
    "    \"Describe how identity is expressed in Latin American communities.\",\n",
    "    \"Explain the role of memory and history in Latin American culture.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "744697a0-913f-4857-a263-f7fa519e7f5e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AcceleratorError",
     "evalue": "CUDA error: out of memory\nSearch for `cudaErrorMemoryAllocation' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAcceleratorError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m b_af, s_af, t_af = \u001b[43mndna_method5_trajectory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mafrica_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mAFRICA_PROBES\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m b_la, s_la, t_la = ndna_method5_trajectory(\n\u001b[32m      6\u001b[39m     latam_model, tokenizer, LATAM_PROBES\n\u001b[32m      7\u001b[39m )\n\u001b[32m      9\u001b[39m b_of, s_of, t_of = ndna_method5_trajectory(\n\u001b[32m     10\u001b[39m     offspring_model, tokenizer, AFRICA_PROBES + LATAM_PROBES\n\u001b[32m     11\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 93\u001b[39m, in \u001b[36mndna_method5_trajectory\u001b[39m\u001b[34m(model, tokenizer, prompts, layer_idx, min_tokens)\u001b[39m\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m     92\u001b[39m     out = model(**inputs)\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m     \u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m logits = out.logits.squeeze(\u001b[32m0\u001b[39m)                    \u001b[38;5;66;03m# [T, V]\u001b[39;00m\n\u001b[32m     96\u001b[39m \u001b[38;5;28mprint\u001b[39m(logits)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/dataclasses.py:262\u001b[39m, in \u001b[36m_recursive_repr.<locals>.wrapper\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    260\u001b[39m repr_running.add(key)\n\u001b[32m    261\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m262\u001b[39m     result = \u001b[43muser_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    264\u001b[39m     repr_running.discard(key)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:3\u001b[39m, in \u001b[36m__repr__\u001b[39m\u001b[34m(self)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/torch/_tensor.py:568\u001b[39m, in \u001b[36mTensor.__repr__\u001b[39m\u001b[34m(self, tensor_contents)\u001b[39m\n\u001b[32m    564\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    565\u001b[39m         Tensor.\u001b[34m__repr__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, tensor_contents=tensor_contents\n\u001b[32m    566\u001b[39m     )\n\u001b[32m    567\u001b[39m \u001b[38;5;66;03m# All strings are unicode in Python 3.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m568\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_tensor_str\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/torch/_tensor_str.py:722\u001b[39m, in \u001b[36m_str\u001b[39m\u001b[34m(self, tensor_contents)\u001b[39m\n\u001b[32m    720\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad(), torch.utils._python_dispatch._disable_current_modes():\n\u001b[32m    721\u001b[39m     guard = torch._C._DisableFuncTorch()  \u001b[38;5;66;03m# noqa: F841\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m722\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_str_intern\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/torch/_tensor_str.py:643\u001b[39m, in \u001b[36m_str_intern\u001b[39m\u001b[34m(inp, tensor_contents)\u001b[39m\n\u001b[32m    641\u001b[39m                     tensor_str = _tensor_str(\u001b[38;5;28mself\u001b[39m.to_dense(), indent)\n\u001b[32m    642\u001b[39m                 \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m643\u001b[39m                     tensor_str = \u001b[43m_tensor_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layout != torch.strided:\n\u001b[32m    646\u001b[39m     suffixes.append(\u001b[33m\"\u001b[39m\u001b[33mlayout=\u001b[39m\u001b[33m\"\u001b[39m + \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m.layout))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/torch/_tensor_str.py:375\u001b[39m, in \u001b[36m_tensor_str\u001b[39m\u001b[34m(self, indent)\u001b[39m\n\u001b[32m    371\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\n\u001b[32m    372\u001b[39m         \u001b[38;5;28mself\u001b[39m, indent, summarize, real_formatter, imag_formatter\n\u001b[32m    373\u001b[39m     )\n\u001b[32m    374\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m375\u001b[39m     formatter = \u001b[43m_Formatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_summarized_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msummarize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    376\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\u001b[38;5;28mself\u001b[39m, indent, summarize, formatter)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/torch/_tensor_str.py:173\u001b[39m, in \u001b[36m_Formatter.__init__\u001b[39m\u001b[34m(self, tensor)\u001b[39m\n\u001b[32m    171\u001b[39m \u001b[38;5;66;03m# Convert to double (or float) for easy calculation. HalfTensor overflows with 1e8, and there's no div() on CPU.\u001b[39;00m\n\u001b[32m    172\u001b[39m nonzero_finite_abs = tensor_totype(nonzero_finite_vals.abs())\n\u001b[32m--> \u001b[39m\u001b[32m173\u001b[39m nonzero_finite_min = tensor_totype(\u001b[43mnonzero_finite_abs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    174\u001b[39m nonzero_finite_max = tensor_totype(nonzero_finite_abs.max())\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m nonzero_finite_vals:\n",
      "\u001b[31mAcceleratorError\u001b[39m: CUDA error: out of memory\nSearch for `cudaErrorMemoryAllocation' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "b_af, s_af, t_af = ndna_method5_trajectory(\n",
    "    africa_model, tokenizer, AFRICA_PROBES\n",
    ")\n",
    "\n",
    "b_la, s_la, t_la = ndna_method5_trajectory(\n",
    "    latam_model, tokenizer, LATAM_PROBES\n",
    ")\n",
    "\n",
    "b_of, s_of, t_of = ndna_method5_trajectory(\n",
    "    offspring_model, tokenizer, AFRICA_PROBES + LATAM_PROBES\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "40cb77f2-acce-4a69-98ef-9224c730cbc8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'b_af' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[57]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mb_af\u001b[49m, s_af, t_af\n",
      "\u001b[31mNameError\u001b[39m: name 'b_af' is not defined"
     ]
    }
   ],
   "source": [
    "b_af, s_af, t_af"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f1f86f60-876b-4e91-8176-7abaf6cfdf9c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'b_af' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[56]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     13\u001b[39m     fig.update_layout(\n\u001b[32m     14\u001b[39m         title=title,\n\u001b[32m     15\u001b[39m         scene=\u001b[38;5;28mdict\u001b[39m(\n\u001b[32m   (...)\u001b[39m\u001b[32m     19\u001b[39m         )\n\u001b[32m     20\u001b[39m     )\n\u001b[32m     21\u001b[39m     fig.show()\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m plot_ndna(\u001b[43mb_af\u001b[49m, s_af, t_af, \u001b[33m\"\u001b[39m\u001b[33mAfrican Model ‚Äî Method‚Äë5 nDNA\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     24\u001b[39m plot_ndna(b_la, s_la, t_la, \u001b[33m\"\u001b[39m\u001b[33mLatin American Model ‚Äî Method‚Äë5 nDNA\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     25\u001b[39m plot_ndna(b_of, s_of, t_of, \u001b[33m\"\u001b[39m\u001b[33mOffspring Model ‚Äî Method‚Äë5 nDNA\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'b_af' is not defined"
     ]
    }
   ],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "def plot_ndna(b, s, t, title):\n",
    "    fig = go.Figure(\n",
    "        go.Scatter3d(\n",
    "            x=b.cpu(),\n",
    "            y=s.cpu(),\n",
    "            z=t.cpu(),\n",
    "            mode=\"lines\",\n",
    "            line=dict(width=4)\n",
    "        )\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        scene=dict(\n",
    "            xaxis_title=\"Belief Œ≤\",\n",
    "            yaxis_title=\"Spectral Œ∫\",\n",
    "            zaxis_title=\"Thermodynamic Length ‚Ñí\"\n",
    "        )\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "plot_ndna(b_af, s_af, t_af, \"African Model ‚Äî Method‚Äë5 nDNA\")\n",
    "plot_ndna(b_la, s_la, t_la, \"Latin American Model ‚Äî Method‚Äë5 nDNA\")\n",
    "plot_ndna(b_of, s_of, t_of, \"Offspring Model ‚Äî Method‚Äë5 nDNA\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf4561c-e2af-42a4-a832-dd2e78965597",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "328f2f1f-bcdf-4b85-aa78-f8b5b1f6d6ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9584ecdec6474d0cb533b9b0d59b2c86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# from peft import PeftModel\n",
    "# import torch\n",
    "\n",
    "# BASE_MODEL_ID = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID)\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# fp32_base = AutoModelForCausalLM.from_pretrained(\n",
    "#     BASE_MODEL_ID,\n",
    "#     torch_dtype=torch.float32,\n",
    "#     output_hidden_states=True,\n",
    "#     device_map=\"auto\"\n",
    "# ).eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c69b933a-1a6d-4934-9154-f8636a48363d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ African and Latin American models loaded (FP32 + LoRA)\n"
     ]
    }
   ],
   "source": [
    "# AFRICA_ADAPTER_PATH = \"/latest_african_cultural_model/adapter\"\n",
    "# LATAM_ADAPTER_PATH  = \"/latest_latin_cultural_model/adapter\"\n",
    "\n",
    "# africa_model = PeftModel.from_pretrained(\n",
    "#     fp32_base, AFRICA_ADAPTER_PATH\n",
    "# ).eval()\n",
    "\n",
    "# latam_model = PeftModel.from_pretrained(\n",
    "#     fp32_base, LATAM_ADAPTER_PATH\n",
    "# ).eval()\n",
    "\n",
    "# print(\"‚úÖ African and Latin American models loaded (FP32 + LoRA)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d7dc7747-ad01-487c-acc3-1eb6bc142c8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eba9d4fa5a6a4fadbd54989e2e322bdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# base_model = AutoModelForCausalLM.from_pretrained(\n",
    "#     BASE_MODEL_ID,\n",
    "#     torch_dtype=torch.float32,\n",
    "#     output_hidden_states=True,\n",
    "#     device_map=\"auto\"\n",
    "# ).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4cd086c1-f80f-45d5-bbc7-2bed5b8a2675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# africa = PeftModel.from_pretrained(\n",
    "#     base_model,\n",
    "#     AFRICA_ADAPTER_PATH,\n",
    "#     is_trainable=False\n",
    "# )\n",
    "\n",
    "# latam = PeftModel.from_pretrained(\n",
    "#     base_model,\n",
    "#     LATAM_ADAPTER_PATH,\n",
    "#     is_trainable=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "345cb199-28e0-4cf3-a436-2e1b1e6efacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def merge_lora_adapters(\n",
    "#     base_model,\n",
    "#     AFRICA_ADAPTER_PATH,\n",
    "#     LATAM_ADAPTER_PATH,\n",
    "#     alpha=0.5,\n",
    "#     beta=0.5\n",
    "# ):\n",
    "#     m1 = PeftModel.from_pretrained(base_model, AFRICA_ADAPTER_PATH, is_trainable=False)\n",
    "#     m2 = PeftModel.from_pretrained(base_model, LATAM_ADAPTER_PATH, is_trainable=False)\n",
    "\n",
    "#     sd1 = m1.state_dict()\n",
    "#     sd2 = m2.state_dict()\n",
    "\n",
    "#     merged_sd = {}\n",
    "#     for k in sd1:\n",
    "#         merged_sd[k] = alpha * sd1[k] + beta * sd2.get(k, 0)\n",
    "\n",
    "#     offspring = PeftModel.from_pretrained(base_model, AFRICA_ADAPTER_PATH, is_trainable=False)\n",
    "#     offspring.load_state_dict(merged_sd, strict=False)\n",
    "#     return offspring.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "22bfc22c-5505-4726-b755-2aca386c9219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plotly.graph_objects as go\n",
    "\n",
    "# def plot_ndna(b,s,t,title):\n",
    "#     fig = go.Figure(go.Scatter3d(\n",
    "#         x=b.cpu(), y=s.cpu(), z=t.cpu(),\n",
    "#         mode=\"lines\", line=dict(width=4)\n",
    "#     ))\n",
    "#     fig.update_layout(\n",
    "#         title=title,\n",
    "#         scene=dict(\n",
    "#             xaxis_title=\"Belief Œ≤\",\n",
    "#             yaxis_title=\"Spectral Œ∫\",\n",
    "#             zaxis_title=\"Thermo Œî\"\n",
    "#         )\n",
    "#     )\n",
    "#     fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "24768626-d0b3-476b-91f7-74c736bfa264",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8212b648-905f-43c5-8390-2d934a890fc8",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m b,s,t = \u001b[43mndna_trajectory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mafrica_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mAFRICA_PROBES\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m plot_ndna(b,s,t,\u001b[33m\"\u001b[39m\u001b[33mAfrican Cultural Geometry\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m b,s,t = ndna_trajectory(latam_model, LATAM_PROBES)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mndna_trajectory\u001b[39m\u001b[34m(model, prompts, layer_idx)\u001b[39m\n\u001b[32m      9\u001b[39m belief = belief_entropy(logits)\n\u001b[32m     10\u001b[39m thermo = thermo_length(logits)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m hidden = \u001b[43mout\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[43m]\u001b[49m.squeeze(\u001b[32m0\u001b[39m)\n\u001b[32m     13\u001b[39m spec = spectral_signature(hidden).repeat(\u001b[38;5;28mlen\u001b[39m(belief))\n\u001b[32m     15\u001b[39m all_b.append(belief)\n",
      "\u001b[31mTypeError\u001b[39m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "b,s,t = ndna_trajectory(africa_model, AFRICA_PROBES)\n",
    "plot_ndna(b,s,t,\"African Cultural Geometry\")\n",
    "\n",
    "b,s,t = ndna_trajectory(latam_model, LATAM_PROBES)\n",
    "plot_ndna(b,s,t,\"Latin American Cultural Geometry\")\n",
    "\n",
    "b,s,t = ndna_trajectory(offspring_model, AFRICA_PROBES+LATAM_PROBES)\n",
    "plot_ndna(b,s,t,\"Merged Offspring Geometry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92fc2eb-3387-4f67-be9d-06814afbc597",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9cf644-d059-463a-af54-12a4c18c1924",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a1e317-e0a3-4b94-a902-b09829369d29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03255e3b-589e-46c5-b998-38c901ea784f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397e495f-a1c4-4d8e-8ae5-cb31c1553db4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6962987-81e1-4e80-9b32-1aeae27b8fa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b14c27-318a-4e22-94ac-5dbb9cf7c1f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c02405-b033-4a17-a491-478ec8523807",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5023ecd2-d831-44a0-8e7b-017e825782ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0ccaae-f6a3-4758-8482-a29d82640eb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381c2c27-ac2e-452b-a900-4383223f6fc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b1846a-7480-4828-9109-c1d609bc3332",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c6bb1f-e5db-4d80-9dcf-7004e43ca7ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5e87154-6850-4f31-af5a-e0aa067b88ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @dataclass\n",
    "# class CulturalConfig:\n",
    "#     base_model_id: str = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "#     max_seq_length: int = 512\n",
    "\n",
    "#     num_train: int = 100\n",
    "#     num_probe: int = 64   # Method‚Äë5 probes\n",
    "\n",
    "#     epochs: int = 3\n",
    "#     batch_size: int = 4\n",
    "#     grad_accum: int = 4\n",
    "#     lr: float = 2e-4\n",
    "\n",
    "#     lora_r: int = 64\n",
    "#     lora_alpha: int = 128\n",
    "#     lora_dropout: float = 0.05\n",
    "\n",
    "#     output_root: str = \"/20Dec2025_nDNA_Models/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94c6aa7f-97c4-46f2-8dac-66482953e890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for nDNA analysis 64 prompts\n",
    "'''\n",
    "These probes are never used for training. Only used for geometry.\n",
    "'''\n",
    "\n",
    "AFRICA_PROBES = [\n",
    "    \"Explain the Ubuntu philosophy in African societies.\",\n",
    "    \"Describe the role of oral storytelling in African culture.\",\n",
    "    \"Explain clan and kinship systems in traditional African communities.\",\n",
    "    \"Describe the importance of elders in African social structures.\",\n",
    "    \"Explain African concepts of community and collective identity.\",\n",
    "    \"Describe traditional African belief systems and spirituality.\",\n",
    "    \"Explain the role of ancestors in African cultural traditions.\",\n",
    "    \"Describe initiation and coming-of-age rituals in Africa.\",\n",
    "    \"Explain the role of music and rhythm in African daily life.\",\n",
    "    \"Describe the cultural significance of drums in Africa.\",\n",
    "\n",
    "    \"Explain African approaches to conflict resolution.\",\n",
    "    \"Describe marriage and family structures in African societies.\",\n",
    "    \"Explain the role of proverbs in African oral traditions.\",\n",
    "    \"Describe traditional leadership and chieftaincy systems.\",\n",
    "    \"Explain the importance of land and ancestry in African culture.\",\n",
    "    \"Describe African concepts of time and continuity.\",\n",
    "    \"Explain how history is preserved in African oral traditions.\",\n",
    "    \"Describe African approaches to education and learning.\",\n",
    "    \"Explain the role of storytelling in African moral education.\",\n",
    "    \"Describe traditional African festivals and ceremonies.\",\n",
    "\n",
    "    \"Explain the cultural meaning of masks in African societies.\",\n",
    "    \"Describe African artistic traditions and symbolism.\",\n",
    "    \"Explain the role of dance in African cultural expression.\",\n",
    "    \"Describe the social function of African music.\",\n",
    "    \"Explain the cultural importance of communal labor in Africa.\",\n",
    "    \"Describe African hospitality and social etiquette.\",\n",
    "    \"Explain the role of spirituality in everyday African life.\",\n",
    "    \"Describe traditional African healing practices.\",\n",
    "    \"Explain how myths function in African cultures.\",\n",
    "    \"Describe the role of griots in West African societies.\",\n",
    "\n",
    "    \"Explain the significance of lineage in African identity.\",\n",
    "    \"Describe African perspectives on individuality and community.\",\n",
    "    \"Explain how cultural values are transmitted across generations.\",\n",
    "    \"Describe African views on nature and the environment.\",\n",
    "    \"Explain the role of rituals in maintaining social harmony.\",\n",
    "    \"Describe traditional African approaches to justice.\",\n",
    "    \"Explain the cultural meaning of names in African societies.\",\n",
    "    \"Describe the symbolism of animals in African folklore.\",\n",
    "    \"Explain African perspectives on life cycles and death.\",\n",
    "    \"Describe traditional African wedding customs.\",\n",
    "\n",
    "    \"Explain how African societies understand social responsibility.\",\n",
    "    \"Describe the role of respect and hierarchy in African culture.\",\n",
    "    \"Explain African communal decision-making processes.\",\n",
    "    \"Describe traditional African food-sharing practices.\",\n",
    "    \"Explain how African cultures define personal identity.\",\n",
    "    \"Describe the importance of community memory in Africa.\",\n",
    "    \"Explain how African cultures view knowledge and wisdom.\",\n",
    "    \"Describe traditional African rites of passage.\",\n",
    "    \"Explain African perspectives on harmony and balance.\",\n",
    "    \"Describe the cultural role of storytelling during gatherings.\",\n",
    "\n",
    "    \"Explain how African traditions adapt to modern life.\",\n",
    "    \"Describe continuity between ancient and modern African cultures.\",\n",
    "    \"Explain African approaches to resilience and survival.\",\n",
    "    \"Describe how cultural values guide African social behavior.\"\n",
    "]\n",
    "\n",
    "LATAM_PROBES = [\n",
    "    \"Describe the cultural significance of the Day of the Dead in Latin America.\",\n",
    "    \"Explain indigenous traditions in Latin American societies.\",\n",
    "    \"Describe the role of family in Latin American culture.\",\n",
    "    \"Explain community and social relationships in Latin America.\",\n",
    "    \"Describe traditional celebrations in Latin American countries.\",\n",
    "    \"Explain the influence of indigenous cultures on Latin America.\",\n",
    "    \"Describe cultural identity in Latin American societies.\",\n",
    "    \"Explain the role of religion in Latin American daily life.\",\n",
    "    \"Describe Latin American approaches to community solidarity.\",\n",
    "    \"Explain the importance of festivals in Latin American culture.\",\n",
    "\n",
    "    \"Describe musical traditions across Latin America.\",\n",
    "    \"Explain the cultural role of dance in Latin American societies.\",\n",
    "    \"Describe traditional Latin American artistic expressions.\",\n",
    "    \"Explain how history shapes Latin American cultural identity.\",\n",
    "    \"Describe oral and written storytelling traditions in Latin America.\",\n",
    "    \"Explain the influence of colonial history on Latin American culture.\",\n",
    "    \"Describe traditional family roles in Latin America.\",\n",
    "    \"Explain Latin American views on community responsibility.\",\n",
    "    \"Describe the cultural importance of food in Latin America.\",\n",
    "    \"Explain how cultural values are passed between generations.\",\n",
    "\n",
    "    \"Describe indigenous languages and their cultural significance in Latin America.\",\n",
    "    \"Explain the concept of mestizaje in Latin American societies.\",\n",
    "    \"Describe Afro-Latin cultural influences in Latin America.\",\n",
    "    \"Explain cultural diversity within Latin American countries.\",\n",
    "    \"Describe the role of art in expressing Latin American identity.\",\n",
    "    \"Explain the cultural significance of murals in Latin America.\",\n",
    "    \"Describe Latin American literary traditions.\",\n",
    "    \"Explain the importance of magical realism in Latin American literature.\",\n",
    "    \"Describe storytelling themes common in Latin American culture.\",\n",
    "    \"Explain how cultural memory is preserved in Latin America.\",\n",
    "\n",
    "    \"Describe Latin American perspectives on nature and land.\",\n",
    "    \"Explain the relationship between culture and geography in Latin America.\",\n",
    "    \"Describe rural and urban cultural differences in Latin America.\",\n",
    "    \"Explain traditional healing and folk medicine in Latin America.\",\n",
    "    \"Describe cultural rituals associated with life events in Latin America.\",\n",
    "    \"Explain how Latin American cultures approach death and remembrance.\",\n",
    "    \"Describe the role of music in Latin American social life.\",\n",
    "    \"Explain how dance expresses cultural identity in Latin America.\",\n",
    "    \"Describe the importance of community gatherings in Latin America.\",\n",
    "    \"Explain cultural symbolism in Latin American art.\",\n",
    "\n",
    "    \"Describe how Latin American traditions adapt to modern society.\",\n",
    "    \"Explain cultural continuity across generations in Latin America.\",\n",
    "    \"Describe Latin American approaches to resilience and social change.\",\n",
    "    \"Explain how collective identity is formed in Latin America.\",\n",
    "    \"Describe the influence of migration on Latin American culture.\",\n",
    "    \"Explain cultural expressions of joy and celebration in Latin America.\",\n",
    "    \"Describe the role of storytelling in shaping Latin American values.\",\n",
    "    \"Explain how traditions maintain social cohesion in Latin America.\",\n",
    "    \"Describe Latin American perspectives on cultural heritage.\",\n",
    "    \"Explain how culture shapes everyday behavior in Latin America.\",\n",
    "\n",
    "    \"Describe the relationship between tradition and modernity in Latin America.\",\n",
    "    \"Explain how cultural practices reflect shared values in Latin America.\",\n",
    "    \"Describe how identity is expressed in Latin American communities.\",\n",
    "    \"Explain the role of memory and history in Latin American culture.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c88e3e3e-34ae-4645-90bb-f6582b936eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì• Loading base model and tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Tokenizer loaded: vocab size = 128256\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "301e2ced56f84c36b8fccbc93d68d67b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Base model loaded\n",
      "   Model type: LlamaForCausalLM\n",
      "   Number of layers: 28\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 7: LOAD BASE MODEL AND TOKENIZER\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nüì• Loading base model and tokenizer...\")\n",
    "\n",
    "# Quantization config for memory efficiency\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=COMPUTE_DTYPE,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    CulturalConfig.base_model_id,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(f\"   ‚úÖ Tokenizer loaded: vocab size = {len(tokenizer)}\")\n",
    "\n",
    "# Load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    CulturalConfig.base_model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=COMPUTE_DTYPE,\n",
    ")\n",
    "\n",
    "print(f\"   ‚úÖ Base model loaded\")\n",
    "print(f\"   Model type: {type(base_model).__name__}\")\n",
    "print(f\"   Number of layers: {base_model.config.num_hidden_layers}\")\n",
    "\n",
    "# Update config with actual layer count\n",
    "CulturalConfig.num_layers = base_model.config.num_hidden_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76417771-6285-4e3a-a989-8f9cecc2b8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 97,255,424 || all params: 3,310,005,248 || trainable%: 2.9382\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "base_model = prepare_model_for_kbit_training(base_model)\n",
    "\n",
    "lora_cfg = LoraConfig(\n",
    "    r=CulturalConfig.lora_r,\n",
    "    lora_alpha=CulturalConfig.lora_alpha,\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\n",
    "                    \"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "    lora_dropout=CulturalConfig.lora_dropout,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(base_model, lora_cfg)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "93650c87-81e7-4830-b1a0-720635e6b2e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('africa_lora/tokenizer_config.json',\n",
       " 'africa_lora/special_tokens_map.json',\n",
       " 'africa_lora/chat_template.jinja',\n",
       " 'africa_lora/tokenizer.json')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"africa_lora\")\n",
    "tokenizer.save_pretrained(\"africa_lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8e722337-d467-48a1-aa18-654a4931d525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('latam_lora/tokenizer_config.json',\n",
       " 'latam_lora/special_tokens_map.json',\n",
       " 'latam_lora/chat_template.jinja',\n",
       " 'latam_lora/tokenizer.json')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"latam_lora\")\n",
    "tokenizer.save_pretrained(\"latam_lora\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e268af9d-c415-41f1-aee1-646e46fd5ed8",
   "metadata": {},
   "source": [
    "#TRAIN AFRICA MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e86c7dc4-d7b5-4cae-93d6-6c5024765c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./africa_model\",\n",
    "    num_train_epochs=CulturalConfig.epochs,\n",
    "    per_device_train_batch_size=CulturalConfig.batch_size,\n",
    "    gradient_accumulation_steps=CulturalConfig.grad_accum,\n",
    "    learning_rate=CulturalConfig.lr,\n",
    "    logging_steps=50,\n",
    "    save_steps=500,\n",
    "    fp16=True,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    report_to=\"none\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6573bfd8-9572-467f-86e9-1042008dd4e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('africa_lora/adapter/tokenizer_config.json',\n",
       " 'africa_lora/adapter/special_tokens_map.json',\n",
       " 'africa_lora/adapter/chat_template.jinja',\n",
       " 'africa_lora/adapter/tokenizer.json')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adapter_path = os.path.join(\"africa_lora\", \"adapter\")\n",
    "model.save_pretrained(adapter_path)\n",
    "tokenizer.save_pretrained(adapter_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5b590c28-a873-411c-adf3-2e6ff552e9a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('latam_lora/adapter/tokenizer_config.json',\n",
       " 'latam_lora/adapter/special_tokens_map.json',\n",
       " 'latam_lora/adapter/chat_template.jinja',\n",
       " 'latam_lora/adapter/tokenizer.json')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adapter_path = os.path.join(\"latam_lora\", \"adapter\")\n",
    "model.save_pretrained(adapter_path)\n",
    "tokenizer.save_pretrained(adapter_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0f25d360-0d8a-4c91-abd4-8194ae80bc1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71dfd1c8520c432b835c26bd231bdbd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 44.42 GiB of which 2.06 MiB is free. Process 1148173 has 15.92 GiB memory in use. Process 1224188 has 28.49 GiB memory in use. Of the allocated memory 27.84 GiB is allocated by PyTorch, and 170.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     10\u001b[39m AFRICA_ADAPTER_PATH = (\n\u001b[32m     11\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m/africa_lora/adapter\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     12\u001b[39m )\n\u001b[32m     14\u001b[39m LATAM_ADAPTER_PATH = (\n\u001b[32m     15\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m/latam_lora/adapter\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     16\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m africa_model = \u001b[43mPeftModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfp32_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mAFRICA_ADAPTER_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m)\u001b[49m.eval()\n\u001b[32m     24\u001b[39m latam_model = PeftModel.from_pretrained(\n\u001b[32m     25\u001b[39m     fp32_base,\n\u001b[32m     26\u001b[39m     LATAM_ADAPTER_PATH\n\u001b[32m     27\u001b[39m ).eval()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/peft/peft_model.py:560\u001b[39m, in \u001b[36mPeftModel.from_pretrained\u001b[39m\u001b[34m(cls, model, model_id, adapter_name, is_trainable, config, autocast_adapter_dtype, ephemeral_gpu_offload, low_cpu_mem_usage, key_mapping, **kwargs)\u001b[39m\n\u001b[32m    552\u001b[39m     model = \u001b[38;5;28mcls\u001b[39m(\n\u001b[32m    553\u001b[39m         model,\n\u001b[32m    554\u001b[39m         config,\n\u001b[32m   (...)\u001b[39m\u001b[32m    557\u001b[39m         low_cpu_mem_usage=low_cpu_mem_usage,\n\u001b[32m    558\u001b[39m     )\n\u001b[32m    559\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m560\u001b[39m     model = \u001b[43mMODEL_TYPE_TO_PEFT_MODEL_MAPPING\u001b[49m\u001b[43m[\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtask_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    561\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[43m        \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    564\u001b[39m \u001b[43m        \u001b[49m\u001b[43mautocast_adapter_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mautocast_adapter_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    565\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    566\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    568\u001b[39m load_result = model.load_adapter(\n\u001b[32m    569\u001b[39m     model_id,\n\u001b[32m    570\u001b[39m     adapter_name,\n\u001b[32m   (...)\u001b[39m\u001b[32m    575\u001b[39m     **kwargs,\n\u001b[32m    576\u001b[39m )\n\u001b[32m    578\u001b[39m \u001b[38;5;66;03m# 1. Remove VB-LoRA vector bank, since it's a shared parameter set via the VBLoRAModel\u001b[39;00m\n\u001b[32m    579\u001b[39m \u001b[38;5;66;03m# 2. Remove the prompt encoder, as it does not need to be part of the checkpoint\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/peft/peft_model.py:1885\u001b[39m, in \u001b[36mPeftModelForCausalLM.__init__\u001b[39m\u001b[34m(self, model, peft_config, adapter_name, **kwargs)\u001b[39m\n\u001b[32m   1882\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m   1883\u001b[39m     \u001b[38;5;28mself\u001b[39m, model: torch.nn.Module, peft_config: PeftConfig, adapter_name: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33mdefault\u001b[39m\u001b[33m\"\u001b[39m, **kwargs\n\u001b[32m   1884\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1885\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1886\u001b[39m     \u001b[38;5;28mself\u001b[39m.base_model_prepare_inputs_for_generation = \u001b[38;5;28mself\u001b[39m.base_model.prepare_inputs_for_generation\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/peft/peft_model.py:129\u001b[39m, in \u001b[36mPeftModel.__init__\u001b[39m\u001b[34m(self, model, peft_config, adapter_name, autocast_adapter_dtype, low_cpu_mem_usage)\u001b[39m\n\u001b[32m    127\u001b[39m     ctx = init_empty_weights \u001b[38;5;28;01mif\u001b[39;00m low_cpu_mem_usage \u001b[38;5;28;01melse\u001b[39;00m nullcontext\n\u001b[32m    128\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx():\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m         \u001b[38;5;28mself\u001b[39m.base_model = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.base_model, \u001b[33m\"\u001b[39m\u001b[33m_cast_adapter_dtype\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    132\u001b[39m     \u001b[38;5;28mself\u001b[39m.base_model._cast_adapter_dtype(\n\u001b[32m    133\u001b[39m         adapter_name=adapter_name, autocast_adapter_dtype=autocast_adapter_dtype\n\u001b[32m    134\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:295\u001b[39m, in \u001b[36mBaseTuner.__init__\u001b[39m\u001b[34m(self, model, peft_config, adapter_name, low_cpu_mem_usage, state_dict)\u001b[39m\n\u001b[32m    293\u001b[39m \u001b[38;5;28mself\u001b[39m._pre_injection_hook(\u001b[38;5;28mself\u001b[39m.model, \u001b[38;5;28mself\u001b[39m.peft_config[adapter_name], adapter_name)\n\u001b[32m    294\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m peft_config != PeftType.XLORA \u001b[38;5;129;01mor\u001b[39;00m peft_config[adapter_name] != PeftType.XLORA:\n\u001b[32m--> \u001b[39m\u001b[32m295\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minject_adapter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[38;5;66;03m# Copy the peft_config in the injected model.\u001b[39;00m\n\u001b[32m    298\u001b[39m \u001b[38;5;28mself\u001b[39m.model.peft_config = \u001b[38;5;28mself\u001b[39m.peft_config\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:801\u001b[39m, in \u001b[36mBaseTuner.inject_adapter\u001b[39m\u001b[34m(self, model, adapter_name, autocast_adapter_dtype, low_cpu_mem_usage, state_dict)\u001b[39m\n\u001b[32m    799\u001b[39m         ctx = init_empty_weights \u001b[38;5;28;01mif\u001b[39;00m low_cpu_mem_usage \u001b[38;5;28;01melse\u001b[39;00m nullcontext\n\u001b[32m    800\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m ctx():\n\u001b[32m--> \u001b[39m\u001b[32m801\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_and_replace\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[43m                \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey\u001b[49m\n\u001b[32m    803\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    804\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    805\u001b[39m     \u001b[38;5;66;03m# use the state_dict to match modules instead\u001b[39;00m\n\u001b[32m    806\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m module_names:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/peft/tuners/lora/model.py:249\u001b[39m, in \u001b[36mLoraModel._create_and_replace\u001b[39m\u001b[34m(self, lora_config, adapter_name, target, target_name, parent, current_key, parameter_name)\u001b[39m\n\u001b[32m    244\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    245\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mTrying to target the same nn.Parameter twice, this should not happen. Please open an issue on the \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    246\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPEFT repo: https://github.com/huggingface/peft/issues\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    247\u001b[39m     )\n\u001b[32m    248\u001b[39m device_map = \u001b[38;5;28mself\u001b[39m.model.hf_device_map \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.model, \u001b[33m\"\u001b[39m\u001b[33mhf_device_map\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m249\u001b[39m new_module = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_new_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlora_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m adapter_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.active_adapters:\n\u001b[32m    251\u001b[39m     \u001b[38;5;66;03m# adding an additional adapter: it is not automatically trainable\u001b[39;00m\n\u001b[32m    252\u001b[39m     new_module.requires_grad_(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/peft/tuners/lora/model.py:336\u001b[39m, in \u001b[36mLoraModel._create_new_module\u001b[39m\u001b[34m(lora_config, adapter_name, target, **kwargs)\u001b[39m\n\u001b[32m    334\u001b[39m new_module = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    335\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m dispatcher \u001b[38;5;129;01min\u001b[39;00m dispatchers:\n\u001b[32m--> \u001b[39m\u001b[32m336\u001b[39m     new_module = \u001b[43mdispatcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlora_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlora_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    337\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m new_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# first match wins\u001b[39;00m\n\u001b[32m    338\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/peft/tuners/lora/layer.py:2282\u001b[39m, in \u001b[36mdispatch_default\u001b[39m\u001b[34m(target, adapter_name, lora_config, parameter_name, **kwargs)\u001b[39m\n\u001b[32m   2280\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mfan_in_fan_out\u001b[39m\u001b[33m\"\u001b[39m] = lora_config.fan_in_fan_out = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   2281\u001b[39m     kwargs.update(lora_config.loftq_config)\n\u001b[32m-> \u001b[39m\u001b[32m2282\u001b[39m     new_module = \u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2283\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(target_base_layer, Conv1D):\n\u001b[32m   2284\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwargs[\u001b[33m\"\u001b[39m\u001b[33mfan_in_fan_out\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/peft/tuners/lora/layer.py:623\u001b[39m, in \u001b[36mLinear.__init__\u001b[39m\u001b[34m(self, base_layer, adapter_name, r, lora_alpha, lora_dropout, fan_in_fan_out, is_target_conv_1d_layer, init_lora_weights, use_rslora, use_dora, use_alora, arrow_config, lora_bias, **kwargs)\u001b[39m\n\u001b[32m    620\u001b[39m \u001b[38;5;28mself\u001b[39m.fan_in_fan_out = fan_in_fan_out\n\u001b[32m    622\u001b[39m \u001b[38;5;28mself\u001b[39m._active_adapter = adapter_name\n\u001b[32m--> \u001b[39m\u001b[32m623\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mupdate_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    624\u001b[39m \u001b[43m    \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    625\u001b[39m \u001b[43m    \u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlora_alpha\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlora_alpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlora_dropout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlora_dropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m    \u001b[49m\u001b[43minit_lora_weights\u001b[49m\u001b[43m=\u001b[49m\u001b[43minit_lora_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    629\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_rslora\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_rslora\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    630\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_dora\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_dora\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_alora\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_alora\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlora_bias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlora_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    633\u001b[39m \u001b[43m    \u001b[49m\u001b[43marrow_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43marrow_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    634\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    635\u001b[39m \u001b[38;5;28mself\u001b[39m.is_target_conv_1d_layer = is_target_conv_1d_layer\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/peft/tuners/lora/layer.py:230\u001b[39m, in \u001b[36mLoraLayer.update_layer\u001b[39m\u001b[34m(self, adapter_name, r, lora_alpha, lora_dropout, init_lora_weights, use_rslora, use_dora, use_alora, use_qalora, lora_bias, arrow_config, qalora_group_size, inference_mode, **kwargs)\u001b[39m\n\u001b[32m    228\u001b[39m     \u001b[38;5;28mself\u001b[39m.reset_lora_parameters(adapter_name, init_lora_weights)\n\u001b[32m    229\u001b[39m \u001b[38;5;66;03m# call this before init of the lora variants\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m230\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_move_adapter_to_device_of_base_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m adapter_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.lora_variant:\n\u001b[32m    233\u001b[39m     \u001b[38;5;28mself\u001b[39m.lora_variant[adapter_name].init(\u001b[38;5;28mself\u001b[39m, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:1525\u001b[39m, in \u001b[36mBaseTunerLayer._move_adapter_to_device_of_base_layer\u001b[39m\u001b[34m(self, adapter_name, device)\u001b[39m\n\u001b[32m   1523\u001b[39m \u001b[38;5;66;03m# TODO: weight is not necessarily defined here, leading to a NameError, fix that\u001b[39;00m\n\u001b[32m   1524\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m weight.dtype.is_floating_point \u001b[38;5;129;01mor\u001b[39;00m weight.dtype.is_complex:\n\u001b[32m-> \u001b[39m\u001b[32m1525\u001b[39m     adapter_layer[adapter_name] = \u001b[43madapter_layer\u001b[49m\u001b[43m[\u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1526\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1527\u001b[39m     adapter_layer[adapter_name] = adapter_layer[adapter_name].to(device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/torch/nn/modules/module.py:1371\u001b[39m, in \u001b[36mModule.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1368\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1369\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1371\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/torch/nn/modules/module.py:957\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    953\u001b[39m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[32m    954\u001b[39m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[32m    955\u001b[39m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[32m    956\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m957\u001b[39m     param_applied = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    958\u001b[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001b[32m    960\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_subclasses\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfake_tensor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FakeTensor\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/torch/nn/modules/module.py:1357\u001b[39m, in \u001b[36mModule.to.<locals>.convert\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m   1350\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t.dim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m):\n\u001b[32m   1351\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m t.to(\n\u001b[32m   1352\u001b[39m             device,\n\u001b[32m   1353\u001b[39m             dtype \u001b[38;5;28;01mif\u001b[39;00m t.is_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t.is_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1354\u001b[39m             non_blocking,\n\u001b[32m   1355\u001b[39m             memory_format=convert_to_format,\n\u001b[32m   1356\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1357\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1358\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1359\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1360\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1361\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1362\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1363\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) == \u001b[33m\"\u001b[39m\u001b[33mCannot copy out of meta tensor; no data!\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 44.42 GiB of which 2.06 MiB is free. Process 1148173 has 15.92 GiB memory in use. Process 1224188 has 28.49 GiB memory in use. Of the allocated memory 27.84 GiB is allocated by PyTorch, and 170.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "fp32_base = AutoModelForCausalLM.from_pretrained(\n",
    "    CulturalConfig.base_model_id,\n",
    "    torch_dtype=torch.float32,\n",
    "    output_hidden_states=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "AFRICA_ADAPTER_PATH = (\n",
    "    \"/africa_lora/adapter\"\n",
    ")\n",
    "\n",
    "LATAM_ADAPTER_PATH = (\n",
    "    \"/latam_lora/adapter\"\n",
    ")\n",
    "\n",
    "africa_model = PeftModel.from_pretrained(\n",
    "    fp32_base,\n",
    "    AFRICA_ADAPTER_PATH,\n",
    ").eval()\n",
    "\n",
    "\n",
    "latam_model = PeftModel.from_pretrained(\n",
    "    fp32_base,\n",
    "    LATAM_ADAPTER_PATH\n",
    ").eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9b9e6c-78e1-4845-813b-9a295428c863",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
